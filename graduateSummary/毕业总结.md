## 1. 大模型微调

近年来，人工智能能力的爆炸式增长主要受到大型语言模型（LLMs）的进展推动。大模型是在大规模文本数据集上训练的神经网络，使它们能够生成类似人类的文本。流行的例子包括由OpenAI创建的GPT-3 （自然语言的生成任务）和由Google创建的BERT （理解自然语言）， T5 (翻译和文本摘要)和ChatGLM 等等。

|         | GPT-1                   | GPT-2                   | GPT-3                     |
--- | --- | ---| --- |
| 模型规模 | 117M                    | 1.5B                    | 175B                      |
| Transformer层数 | 12                   | 48                      | 96                        |
| 预训练数据集 | Books1 和英语维基百科 | Books1, Books2, 和英语维基百科 | Common Crawl, WebText2, Books1, Books2， 和英语维基百科 |

## 1.2 为什么要做大模型微调

- 第一完成特定领域的任务和新的任务。大模型模型在初始的训练数据是公开的数据集不是企业的私有数据，因此训练出来的大模型具有通用的知识很能力，它可以回答通用的问题，但缺乏专业的领域知识，比如医疗，金融，销售，教育，科研。这就是微调发挥作用的地方——即定制大模型以适应特定任务或领域的过程。微调通过进一步在自己的数据上进行训练，将通用的LLM调整为专业领域的模型。
 
- 第二节约资源提高效率。因为从头开始训练大型语言模型在计算资源和时间方面都非常耗费资源。利用预训练模型中嵌入的现有知识使得能够在特定任务上实现高性能，而所需的数据和计算要求大大降低。

## 3. 大模型微调的主要方法

微调大型语言模型（LLM）涉及一种监督学习过程。在这种方法中，利用包含标记示例的数据集来调整模型的权重，增强其在特定任务中的能力。微调过程中使用的技术包括但不局限于：

- 全面微调（指令微调）：指令微调是一种通过训练模型对指令的响应进行引导来提高模型在各种任务中性能的策略。数据集的选择至关重要，并且应针对特定任务进行定制，例如摘要或翻译。这种被称为全面微调的方法更新所有模型权重，创建一个具有改进能力的新版本。然而，它需要足够的内存和计算资源，类似于预训练，以处理梯度、优化器和其他组件在训练期间的存储和处理。

- 参数高效微调（PEFT）是指令微调的一种形式，比全面微调要高效得多。训练语言模型，特别是用于全面LLM微调，需要大量的计算资源。内存分配不仅需要用于存储模型，还需要用于训练过程中的基本参数，这对于简单的硬件构成了挑战。PEFT通过仅更新一部分参数，有效地“冻结”其余参数来解决这个问题。这减少了可训练参数的数量，使内存需求更易管理，并防止了灾难性遗忘。与全面微调不同，PEFT保持了原始LLM权重，避免了以前学习的信息的丢失。这种方法在处理微调多个任务时处理存储问题方面非常有益。有各种方式可以实现参数高效微调。低秩调整LoRA和QLoRA是最广泛使用且有效的方法。

### 3.1 prompt engineering

提示词修改包括硬提示调整、软提示调整和前缀调整。

Prefix-tuning 冻结大语言模型参数，优化小规模的连续任务特定向量 prefix。在prefix-tuning过程中，prefix是一组与语言模型一起训练的自由参数。Prefix-tuning的目标是找到一个上下文，引导大语言模型来解决特定任务的文本生成任务。

![Prefix Tunning](.\prefixtunning.png)

Prompt tuning是另一种用于使预训练语言模型适应特定下游任务的PEFT技术。与传统的“模型调整”方法不同，传统方法是为每个任务调整所有预训练模型参数，而Prompt tunning通过反向传播学习 soft prompt，这些 prompts可以通过特定任务的有标记的数据例子来微调。Prompt tuning优于GPT-3的少样本学习，并随着模型大小的增加而变得更具竞争力。对于每个任务，它需要存储一个小的任务特定提示，这使得可以用单个冻结模型来训练多个下游任务，而不像模型调整需要为每个任务制作整个预训练模型的特定副本。

Prompt tunning是 Prefix tunning 的一个更简单的变体。一些向量被预先添加到输入层的序列开头。对于一个输入，嵌入层将每个token转换为其相应的word embedding，并将 prefix 嵌入添加到token embeddings序列中。然后，预训练的Transformer层将处理embedding序列，就像处理普通序列一样。在微调过程中只调整prefix embeedings，而transformer模型的其余部分保持不变。

Prompt Tuning 主要贡献：
 - 直观性：Prompt tuning 使用直观的语言提示来引导模型，使其更易于理解和操作。
 - 适用性：这种方法特别适用于那些预训练模型已经掌握了大量通用知识的情况，通过简单的提示就能激发特定的响应。
 - 微调成本低：prompt tuning 可以在微调时减少所需的计算资源，同时保持良好的性能

Prompt Tuning 训练方法：
 - 设计提示：根据任务选择硬提示（固定文本）或软提示（可训练向量）作为输入。
 - 融入输入：硬提示直接加入文本，软提示作为向量加入序列。
 - 训练过程：硬提示下全面微调模型；软提示下只调整提示向量，其他参数不变。
 - 执行任务：训练后模型用于NLP任务（如问答、摘要），输出由提示引导。

![Prompt Tunning](.\prompttunning.png)

### 3.2 adapter
Adapter 嵌入Transformer 网络：
  - 在两个FNN层后增加Adapter 层
  - Adapter 内部学习降维后特征，减少参数
  - 使用skip-connection，最差退化为identity
  - 提升微调效率和稳定性，可复用性增强
 
Adapter方法是一种特殊类型的子模块，可以添加到预训练语言模型中，在微调期间修改其隐藏表示。通过在Transformer结构中的多头注意力和前馈层后面插入适配器，我们可以在微调期间仅更新适配器中的参数，同时保持其余模型参数冻结。

采用适配器可以是一个简单的过程。只需将适配器添加到每个Transformer中，并在预训练模型的顶部放置一个分类器层。通过更新适配器和分类器头的参数，我们可以提高预训练模型在特定任务上的性能，而不需更新整个模型。

适配器模块包括两个前馈投影层，它们通过一个非线性激活层连接。还有一个绕过前馈层的跳跃连接。

如果我们将适配器放置在多头注意力层后面，那么适配器层的输入是由多头注意力层计算的隐藏表示h。在这里，h在适配器层中采取两条不同的路径；一条是跳跃连接，它保持输入不变，另一条是通过前馈层的路径。

使用适配器进行微调
最初，第一个前馈层将h投影到一个低维空间中。这个空间的维度小于h。在此之后，输入通过一个非线性激活函数，并且第二个前馈层将其再次投影到h的维度。两种方式获得的结果被加在一起，以获得适配器模块的最终输出。

跳跃连接保留了适配器的原始输入h，而前馈路径生成一个增量变化，表示为Δh，基于原始h。通过将从前馈层获得的增量变化Δh与来自上一层的原始h相加，适配器修改了预训练模型计算的隐藏表示。这使得适配器可以修改预训练模型的隐藏表示，从而改变其特定任务的输出。

![Adapter](.\adapter.png)

### 3.3 lora

LoRA是一种改进的微调方法，与微调预训练的大型语言模型的所有权重相比，该方法用两个近似该大型矩阵的较小矩阵。这些矩阵构成LoRA适配器。然后，这个微调后的适配器被加载到预训练模型中，并用于推理。

为了使微调更加高效，LoRA的方法是通过低秩分解将权重更新表示为两个较小的矩阵（称为更新矩阵）。这些新矩
阵可以在适应新数据的同时保持整体变化数量较少进行训练。原始权重矩阵保持冻结状态，并且不再接受任何进一步的
调整。最终结果是通过将原始权重和适应后的权重进行组合得到。

在针对特定任务或用例进行LoRA微调之后，其结果是原始LLM保持不变，而一个规模相当小的“LoRA适配器”出现，通常表示为原始LLM大小的个位数百分比（以MB而不是GB为单位）。

在推理过程中，LoRA适配器必须与其原始LLM结合使用。其优势在于许多LoRA适配器能够重用原始LLM，从而在处理多个任务和用例时降低了总体内存需求。

- LoRA通过修改模型的权重矩阵，直接影响模型的内部表示和处理机制，而不仅仅是输入层级。这意味着LoRA能够在模型的更深层次上产生影响，可能导致更有效的学习和适应性。
- 无需牺牲输入空间：
  - Soft prompts通常需要占用模型的输入空间，这在有限的序列长度下可能限制了其他实际输入内容的长度。
  - LoRA不依赖于Prompt调整方法，避免了相关的限制，因此不会影响模型能处理的输入长度

![Lora Fine Tuning](.\lora.png)

### 3.4 q-lora

QLoRA是LoRA的一个更节省内存的迭代版本。 QLoRA通过将LoRA适配器（较小的矩阵）的权重量化到更低的精度（例如4位而不是8位），进一步推进了LoRA。这进一步减少了内存占用和存储需求。 在QLoRA中，预训练模型以量化的4位权重加载到GPU内存中，而不是LoRA中使用的8位。 尽管降低了比特精度，但QLoRA与LoRA保持了可比的有效性水平。

与LoRA不同，LoRA保持基础模型网络的完整,连接附加数据，而QLoRA引入了16位网络节点量化为4位，并采用了分页机制用于交换二进制数据以处理具有有限内存的大型模型。尽管从16位减少到4位会有一些信息丢失，但保持了16字节微调任务的性能. QLoRA可以在单个48G的GPU显卡上微调65B的参数模型。QLoRA通过冻结的int4量化预训练语言模型反向传播梯度到低秩适配器LoRA来实现微调。

![Qlora](.\qlora.png)

### 3.5 p-tuning

P-Tuning的创新之处在于将提示（Prompt）转化为可学习的嵌入层（Embedding Layer），但直接对嵌入层参数进行优化时面临两大挑战：
 - 离散性（Discreteness）：已经通过预训练优化过的正常语料嵌入层与直接对输入提示嵌入进行随机初始化训练相比, 可能会导致后者陷入局部最优解。
 - 关联性（Association）：这种方法难以有效捕捉提示嵌入之间的相互关系。

P-Tuning和 Prefix-Tuning  主要区别在于：
 - Prefix Tuning 类似于模仿指令，通过在模型开头加入额外的嵌入（embedding），而P-Tuning的嵌入位置更为灵活。
 - Prefix Tuning 在每个注意力层增加前缀嵌入来引入额外参数，并用多层感知机（MLP）进行初始
化；相比之下，P-Tuning  仅在输入时加入嵌入，并通过长短期记忆网络（LSTM）加MLP进行初始化。

P-Tuning  在小模型上性能不佳。P-Tuning v2  旨在使提示调整（Prompt Tuning）在不同规模
的预训练模型上，针对各种下游任务都能达到类似全面微调（Fine-tuning）的效果。

之前的方法在以下两方面有所限制：
 - 模型规模差异：在大型预训练模型中，Prompt Tuning 和P-Tuning  能取得与全面微调相似的效果，但在参数较少的模型上则表现不佳。
 - 任务类型差异：无论是Prompt Tuning 还是P-Tuning，在序列标注任务上的表现都较差。

P-Tuning的核心技术：
- 重参数化（Reparameterization）：
  - 在Prefix Tuning和P-tuning中，多层感知机（MLP）被用来构造可训练的嵌入（embedding）。
  - P-Tuning v2 的研究发现，针对不同的任务和数据集，这种方法可能产生相反的效果，特别是在自然语言理解领域。
- 提示长度（Prompt Length）：
  - 不同任务对应的最优提示长度（Prompt Length）是不一样的。
  - 例如，在简单的分类任务中，长度为20的提示可能是最佳选择；而对于更复杂的任务，则需要更长的提示长度。
- 多任务学习（Multi-task Learning）：
  - 对于P-Tuning v2 而言，多任务学习是可选的，但它可以提供更好的参数初始化，从而进一步提升模型性能。
- 分类头（Classification Head）：
  - 在Prompt Tuning中，使用语言模型（LM）头来预测动词是核心思路。
  - 然而，P-Tuning v2 的研究发现，在完整数据集上这种做法并非必要，且与序列标记任务不兼容。
  - 因此，P-Tuning v2 采用类似BERT的方式，在第一个token处应用随机初始化的分类头。

![p-tunning](.\ptunning.png)

### 3.6 deepspeed 

DeepSpeed 是一个开源深度学习优化库，旨在提高大模型训练和运行效率，以支持数千亿~万亿参数的超大语言模型。在huggingface上有lora, qlora, awq, deepspeed的实现。

## 4. 大模型微调的训练步骤

有了预训练的大预言模型、训练数据和任务，我们可以微调基础模型了：

- 使用随机梯度下降和反向传播设置批处理的训练，以更新权重。
- 与原始预训练相比，训练epoch比少 — 通常只需要10-20个epoches。
- 使用较小的学习率，如1e-5或5e-6。
- 随着时间的推移监视训练和验证损失，以判断收敛情况。
- 当验证性能达到平稳水平并且过拟合风险增加时停止。

References:
1. 彭靖田，AI大模型微调训练营2024
2. https://www.leewayhertz.com/parameter-efficient-fine-tuning/
3. https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07#:~:text=Fine%2Dtuning%20LLM%20involves%20the,the%20GPT%20series%20by%20OpenAI
4. https://arxiv.org/ftp/arxiv/papers/2401/2401.02981.pdf