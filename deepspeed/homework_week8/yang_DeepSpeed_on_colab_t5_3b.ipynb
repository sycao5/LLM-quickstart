{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsT5mHM6VTpt"
      },
      "source": [
        "\n",
        "\n",
        "# transformers + deepspeed CLI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr8bCfITLOQe",
        "outputId": "ece9a34c-33a4-4786-fcfc-2dd156f56cd5"
      },
      "source": [
        "# Free colab seems to give different amount of general RAM to different users or even the same users at different times.\n",
        "\n",
        "!free -h"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            83Gi       785Mi        80Gi       1.0Mi       2.5Gi        81Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ1iecs6SWgk",
        "outputId": "9e633279-e10f-40a7-9fc2-873d29ecab93"
      },
      "source": [
        "# check which nvidia drivers and cuda version is running\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 12 01:21:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Need ninja when build deepspeed from source code\n",
        "\n",
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDLb7i3v_KZ0",
        "outputId": "d1b662f7-a640-4bec-ffa6-42b18af8ed11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m297.0/307.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 源代码安装 DeepSpeed\n",
        "# 根据你的 GPU 实际情况（查看方法见前一页），设置参数 TORCH_CUDA_ARCH_LIST；\n",
        "# 如果你需要使用 NVMe Offload，设置参数  DS_BUILD_UTILS=1；\n",
        "# 如果你需要使用 CPU Offload 优化器参数，设置参数 DS_BUILD_CPU_ADAM=1；\n",
        "!git clone https://github.com/microsoft/DeepSpeed/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFdWTCrspEGp",
        "outputId": "670e501e-656e-4c59-f774-e4dff38d3185"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepSpeed'...\n",
            "remote: Enumerating objects: 42409, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 42409 (delta 23), reused 33 (delta 17), pack-reused 42362\u001b[K\n",
            "Receiving objects: 100% (42409/42409), 202.05 MiB | 14.42 MiB/s, done.\n",
            "Resolving deltas: 100% (30590/30590), done.\n",
            "Updating files: 100% (1499/1499), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "\n",
        "cd DeepSpeed\n",
        "rm -rf build\n",
        "TORCH_CUDA_ARCH_LIST=\"7.5\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install -e . \\\n",
        "--global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v \\\n",
        "--disable-pip-version-check 2>&1 | tee build.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRl_zWnNEKHS",
        "outputId": "f59b7388-a88f-42ce-90d6-391e10b2fd3e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "DEPRECATION: --build-option and --global-option are deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11859\n",
            "WARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option. \n",
            "Obtaining file:///content/DeepSpeed\n",
            "  Preparing metadata (setup.py): started\n",
            "  Running command python setup.py egg_info\n",
            "  [2024-02-12 01:44:47,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "  [2024-02-12 01:44:47,200] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "  DS_BUILD_OPS=0\n",
            "  \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "  \u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "  \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "  Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "  Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "  \u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "  \u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
            "  \u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
            "  Install Ops={'async_io': False, 'fused_adam': False, 'cpu_adam': 1, 'cpu_adagrad': False, 'cpu_lion': False, 'evoformer_attn': False, 'fused_lamb': False, 'fused_lion': False, 'inference_core_ops': False, 'cutlass_ops': False, 'transformer_inference': False, 'quantizer': False, 'ragged_device_ops': False, 'ragged_ops': False, 'random_ltd': False, 'sparse_attn': False, 'spatial_inference': False, 'transformer': False, 'stochastic_transformer': False}\n",
            "  version=0.13.2+18179807, git_hash=18179807, git_branch=master\n",
            "  install_requires=['hjson', 'ninja', 'numpy', 'packaging>=20.0', 'psutil', 'py-cpuinfo', 'pydantic', 'pynvml', 'torch', 'tqdm']\n",
            "  compatible_ops={'async_io': False, 'fused_adam': True, 'cpu_adam': True, 'cpu_adagrad': True, 'cpu_lion': True, 'evoformer_attn': False, 'fused_lamb': True, 'fused_lion': True, 'inference_core_ops': True, 'cutlass_ops': True, 'transformer_inference': True, 'quantizer': True, 'ragged_device_ops': True, 'ragged_ops': True, 'random_ltd': True, 'sparse_attn': False, 'spatial_inference': True, 'transformer': True, 'stochastic_transformer': True, 'deepspeed_not_implemented': False}\n",
            "  ext_modules=[<setuptools.extension.Extension('deepspeed.ops.adam.cpu_adam_op') at 0x797f1e0de200>]\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  warning: no files found matching 'deepspeed/inference/v2/kernels/ragged_ops/libs/*.so'\n",
            "  warning: no files found matching 'deepspeed/inference/v2/kernels/cutlass_ops/libs/*.so'\n",
            "  warning: no files found matching '*.hip' under directory 'deepspeed'\n",
            "  warning: no files found matching '*.cc' under directory 'deepspeed'\n",
            "  warning: no files found matching '*.tr' under directory 'csrc'\n",
            "  warning: no files found matching '*.cc' under directory 'csrc'\n",
            "  warning: no files found matching '*.py' under directory 'benchmarks'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-yevftdnz/deepspeed.egg-info/SOURCES.txt'\n",
            "  deepspeed build time = 0.4181337356567383 secs\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting hjson (from deepspeed==0.13.2+18179807)\n",
            "  Downloading hjson-3.1.0.tar.gz (40 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 2.8 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Running command python setup.py egg_info\n",
            "  Warning: 'classifiers' should be a list, got type 'filter'\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/entry_points.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  no previously-included directories found matching '#*'\n",
            "  adding license file 'LICENSE.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-58czl_wz/hjson.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (1.11.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (9.0.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (2.6.1)\n",
            "Collecting pynvml (from deepspeed==0.13.2+18179807)\n",
            "  Downloading pynvml-11.5.0.tar.gz (70 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70.2/70.2 kB 7.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Running command python setup.py egg_info\n",
            "  /usr/local/lib/python3.10/dist-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          The license_file parameter is deprecated, use license_files instead.\n",
            "\n",
            "          This deprecation is overdue, please update your project and remove deprecated\n",
            "          calls to avoid build errors in the future.\n",
            "\n",
            "          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    parsed = self.parsers.get(option_name, lambda x: x)(value)\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/dependency_links.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-n930mmph/pynvml.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.13.2+18179807) (4.66.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.2+18179807) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.2+18179807) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.13.2+18179807) (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.13.2+18179807) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.13.2+18179807) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.13.2+18179807) (1.3.0)\n",
            "Building wheels for collected packages: hjson, pynvml\n",
            "  Building wheel for hjson (setup.py): started\n",
            "  Running command python setup.py bdist_wheel\n",
            "  Warning: 'classifiers' should be a list, got type 'filter'\n",
            "  running build_ext\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/hjson\n",
            "  copying hjson/tool.py -> build/lib/hjson\n",
            "  copying hjson/__init__.py -> build/lib/hjson\n",
            "  copying hjson/ordered_dict.py -> build/lib/hjson\n",
            "  copying hjson/scanner.py -> build/lib/hjson\n",
            "  copying hjson/encoderH.py -> build/lib/hjson\n",
            "  copying hjson/encoder.py -> build/lib/hjson\n",
            "  copying hjson/decoder.py -> build/lib/hjson\n",
            "  copying hjson/compat.py -> build/lib/hjson\n",
            "  creating build/lib/hjson/tests\n",
            "  copying hjson/tests/test_check_circular.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_for_json.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_indent.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_decimal.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_errors.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_hjson.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_dump.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_encode_basestring_ascii.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_tuple.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_float.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_recursion.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/__init__.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_unicode.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_default.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_pass1.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_scanstring.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_fail.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_bitsize_int_as_string.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_pass2.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_namedtuple.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_pass3.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_tool.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_separators.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_decode.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_item_sort_key.py -> build/lib/hjson/tests\n",
            "  copying hjson/tests/test_bigint_as_string.py -> build/lib/hjson/tests\n",
            "  /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please avoid running ``setup.py`` directly.\n",
            "          Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "          other standards-based tools.\n",
            "\n",
            "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self.initialize_options()\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/tool.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/__init__.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/ordered_dict.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/scanner.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/encoderH.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/encoder.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  creating build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_check_circular.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_for_json.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_indent.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_decimal.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_errors.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_hjson.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_dump.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_encode_basestring_ascii.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_tuple.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_float.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_recursion.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/__init__.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_unicode.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_default.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_pass1.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_scanstring.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_fail.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_bitsize_int_as_string.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_pass2.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_namedtuple.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_pass3.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_tool.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_separators.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_decode.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_item_sort_key.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/tests/test_bigint_as_string.py -> build/bdist.linux-x86_64/wheel/hjson/tests\n",
            "  copying build/lib/hjson/decoder.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  copying build/lib/hjson/compat.py -> build/bdist.linux-x86_64/wheel/hjson\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing hjson.egg-info/PKG-INFO\n",
            "  writing dependency_links to hjson.egg-info/dependency_links.txt\n",
            "  writing entry points to hjson.egg-info/entry_points.txt\n",
            "  writing top-level names to hjson.egg-info/top_level.txt\n",
            "  reading manifest file 'hjson.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  no previously-included directories found matching '#*'\n",
            "  adding license file 'LICENSE.txt'\n",
            "  writing manifest file 'hjson.egg-info/SOURCES.txt'\n",
            "  Copying hjson.egg-info to build/bdist.linux-x86_64/wheel/hjson-3.1.0-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/hjson-3.1.0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-a14prz14/hjson-3.1.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'hjson/__init__.py'\n",
            "  adding 'hjson/compat.py'\n",
            "  adding 'hjson/decoder.py'\n",
            "  adding 'hjson/encoder.py'\n",
            "  adding 'hjson/encoderH.py'\n",
            "  adding 'hjson/ordered_dict.py'\n",
            "  adding 'hjson/scanner.py'\n",
            "  adding 'hjson/tool.py'\n",
            "  adding 'hjson/tests/__init__.py'\n",
            "  adding 'hjson/tests/test_bigint_as_string.py'\n",
            "  adding 'hjson/tests/test_bitsize_int_as_string.py'\n",
            "  adding 'hjson/tests/test_check_circular.py'\n",
            "  adding 'hjson/tests/test_decimal.py'\n",
            "  adding 'hjson/tests/test_decode.py'\n",
            "  adding 'hjson/tests/test_default.py'\n",
            "  adding 'hjson/tests/test_dump.py'\n",
            "  adding 'hjson/tests/test_encode_basestring_ascii.py'\n",
            "  adding 'hjson/tests/test_errors.py'\n",
            "  adding 'hjson/tests/test_fail.py'\n",
            "  adding 'hjson/tests/test_float.py'\n",
            "  adding 'hjson/tests/test_for_json.py'\n",
            "  adding 'hjson/tests/test_hjson.py'\n",
            "  adding 'hjson/tests/test_indent.py'\n",
            "  adding 'hjson/tests/test_item_sort_key.py'\n",
            "  adding 'hjson/tests/test_namedtuple.py'\n",
            "  adding 'hjson/tests/test_pass1.py'\n",
            "  adding 'hjson/tests/test_pass2.py'\n",
            "  adding 'hjson/tests/test_pass3.py'\n",
            "  adding 'hjson/tests/test_recursion.py'\n",
            "  adding 'hjson/tests/test_scanstring.py'\n",
            "  adding 'hjson/tests/test_separators.py'\n",
            "  adding 'hjson/tests/test_tool.py'\n",
            "  adding 'hjson/tests/test_tuple.py'\n",
            "  adding 'hjson/tests/test_unicode.py'\n",
            "  adding 'hjson-3.1.0.dist-info/LICENSE.txt'\n",
            "  adding 'hjson-3.1.0.dist-info/METADATA'\n",
            "  adding 'hjson-3.1.0.dist-info/WHEEL'\n",
            "  adding 'hjson-3.1.0.dist-info/entry_points.txt'\n",
            "  adding 'hjson-3.1.0.dist-info/top_level.txt'\n",
            "  adding 'hjson-3.1.0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for hjson (setup.py): finished with status 'done'\n",
            "  Created wheel for hjson: filename=hjson-3.1.0-py3-none-any.whl size=54019 sha256=89bbf09d8a50b3b9eb4e75afc3d892c9005e83ea6501ce60170348d6e6bede1e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x51hs0d6/wheels/6c/d0/f7/31b7b04c4e42ecea4f69520870a8421be8a166f8e0b2d9ab92\n",
            "  Building wheel for pynvml (setup.py): started\n",
            "  Running command python setup.py bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          The license_file parameter is deprecated, use license_files instead.\n",
            "\n",
            "          This deprecation is overdue, please update your project and remove deprecated\n",
            "          calls to avoid build errors in the future.\n",
            "\n",
            "          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    parsed = self.parsers.get(option_name, lambda x: x)(value)\n",
            "  running build_ext\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/pynvml\n",
            "  copying pynvml/__init__.py -> build/lib/pynvml\n",
            "  copying pynvml/_version.py -> build/lib/pynvml\n",
            "  copying pynvml/smi.py -> build/lib/pynvml\n",
            "  copying pynvml/nvml.py -> build/lib/pynvml\n",
            "  UPDATING build/lib/pynvml/_version.py\n",
            "  set build/lib/pynvml/_version.py to '11.5.0'\n",
            "  /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please avoid running ``setup.py`` directly.\n",
            "          Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "          other standards-based tools.\n",
            "\n",
            "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self.initialize_options()\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/pynvml\n",
            "  copying build/lib/pynvml/__init__.py -> build/bdist.linux-x86_64/wheel/pynvml\n",
            "  copying build/lib/pynvml/_version.py -> build/bdist.linux-x86_64/wheel/pynvml\n",
            "  copying build/lib/pynvml/smi.py -> build/bdist.linux-x86_64/wheel/pynvml\n",
            "  copying build/lib/pynvml/nvml.py -> build/bdist.linux-x86_64/wheel/pynvml\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing pynvml.egg-info/PKG-INFO\n",
            "  writing dependency_links to pynvml.egg-info/dependency_links.txt\n",
            "  writing top-level names to pynvml.egg-info/top_level.txt\n",
            "  reading manifest file 'pynvml.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE.txt'\n",
            "  writing manifest file 'pynvml.egg-info/SOURCES.txt'\n",
            "  Copying pynvml.egg-info to build/bdist.linux-x86_64/wheel/pynvml-11.5.0-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/pynvml-11.5.0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-49mw4rb0/pynvml-11.5.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'pynvml/__init__.py'\n",
            "  adding 'pynvml/_version.py'\n",
            "  adding 'pynvml/nvml.py'\n",
            "  adding 'pynvml/smi.py'\n",
            "  adding 'pynvml-11.5.0.dist-info/LICENSE.txt'\n",
            "  adding 'pynvml-11.5.0.dist-info/METADATA'\n",
            "  adding 'pynvml-11.5.0.dist-info/WHEEL'\n",
            "  adding 'pynvml-11.5.0.dist-info/top_level.txt'\n",
            "  adding 'pynvml-11.5.0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for pynvml (setup.py): finished with status 'done'\n",
            "  Created wheel for pynvml: filename=pynvml-11.5.0-py3-none-any.whl size=53135 sha256=79456190bd29d51fae44ffe2f6cbde18c206ba4d8c093df95f11be9c7dee431f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x51hs0d6/wheels/d1/ec/f0/c065153a5242b9e98734cd4cc5815069bd3d1f8e5bdfdb25e0\n",
            "Successfully built hjson pynvml\n",
            "Installing collected packages: hjson, pynvml, deepspeed\n",
            "  changing mode of /usr/local/bin/hjson to 755\n",
            "  Running setup.py develop for deepspeed\n",
            "    Running command python setup.py develop\n",
            "    [2024-02-12 01:45:01,161] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "    [2024-02-12 01:45:03,776] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "    DS_BUILD_OPS=0\n",
            "    \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
            "    \u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
            "    \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
            "    Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "    Installed CUDA version 12.2 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n",
            "    \u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
            "    \u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n",
            "    \u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
            "    Install Ops={'async_io': False, 'fused_adam': False, 'cpu_adam': 1, 'cpu_adagrad': False, 'cpu_lion': False, 'evoformer_attn': False, 'fused_lamb': False, 'fused_lion': False, 'inference_core_ops': False, 'cutlass_ops': False, 'transformer_inference': False, 'quantizer': False, 'ragged_device_ops': False, 'ragged_ops': False, 'random_ltd': False, 'sparse_attn': False, 'spatial_inference': False, 'transformer': False, 'stochastic_transformer': False}\n",
            "    version=0.13.2+18179807, git_hash=18179807, git_branch=master\n",
            "    install_requires=['hjson', 'ninja', 'numpy', 'packaging>=20.0', 'psutil', 'py-cpuinfo', 'pydantic', 'pynvml', 'torch', 'tqdm']\n",
            "    compatible_ops={'async_io': False, 'fused_adam': True, 'cpu_adam': True, 'cpu_adagrad': True, 'cpu_lion': True, 'evoformer_attn': False, 'fused_lamb': True, 'fused_lion': True, 'inference_core_ops': True, 'cutlass_ops': True, 'transformer_inference': True, 'quantizer': True, 'ragged_device_ops': True, 'ragged_ops': True, 'random_ltd': True, 'sparse_attn': False, 'spatial_inference': True, 'transformer': True, 'stochastic_transformer': True, 'deepspeed_not_implemented': False}\n",
            "    ext_modules=[<setuptools.extension.Extension('deepspeed.ops.adam.cpu_adam_op') at 0x7df1595358d0>]\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "      warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "    building 'deepspeed.ops.adam.cpu_adam_op' extension\n",
            "    creating build\n",
            "    creating build/temp.linux-x86_64-cpython-310\n",
            "    creating build/temp.linux-x86_64-cpython-310/csrc\n",
            "    creating build/temp.linux-x86_64-cpython-310/csrc/adam\n",
            "    creating build/temp.linux-x86_64-cpython-310/csrc/common\n",
            "    x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -Icsrc/includes -I/usr/local/cuda/include -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/adam/cpu_adam.cpp -o build/temp.linux-x86_64-cpython-310/csrc/adam/cpu_adam.o -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cpu_adam_op -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "    In file included from csrc/includes/cpu_adam.h:14,\n",
            "                     from csrc/adam/cpu_adam.cpp:6:\n",
            "    csrc/includes/simd.h:77: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       77 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:84: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       84 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:90: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       90 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:98: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       98 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:106: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      106 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:114: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      114 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:120: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      120 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:126: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      126 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:132: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      132 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:138: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      138 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:144: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      144 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:150: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      150 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:156: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      156 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:162: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      162 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:168: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      168 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:176: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      176 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:182: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      182 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:188: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      188 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:194: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      194 | #pragma unroll\n",
            "          |\n",
            "    x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -Icsrc/includes -I/usr/local/cuda/include -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/adam/cpu_adam_impl.cpp -o build/temp.linux-x86_64-cpython-310/csrc/adam/cpu_adam_impl.o -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cpu_adam_op -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "    In file included from csrc/includes/cpu_adam.h:14,\n",
            "                     from csrc/adam/cpu_adam_impl.cpp:12:\n",
            "    csrc/includes/simd.h:77: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       77 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:84: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       84 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:90: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       90 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:98: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "       98 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:106: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      106 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:114: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      114 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:120: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      120 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:126: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      126 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:132: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      132 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:138: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      138 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:144: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      144 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:150: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      150 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:156: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      156 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:162: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      162 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:168: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      168 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:176: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      176 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:182: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      182 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:188: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      188 | #pragma unroll\n",
            "          |\n",
            "    csrc/includes/simd.h:194: warning: ignoring ‘#pragma unroll ’ [-Wunknown-pragmas]\n",
            "      194 | #pragma unroll\n",
            "          |\n",
            "    csrc/adam/cpu_adam_impl.cpp: In member function ‘void Adam_Optimizer::Step_1(float*, float*, float*, float*, size_t, ds_half_precision_t*, bool)’:\n",
            "    csrc/adam/cpu_adam_impl.cpp:67:9: warning: ‘params_cast_h’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
            "       67 | #pragma omp parallel for\n",
            "          |         ^~~\n",
            "    csrc/adam/cpu_adam_impl.cpp:67:9: warning: ‘grads_cast_h’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
            "    /usr/local/cuda/bin/nvcc -Icsrc/includes -I/usr/local/cuda/include -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/common/custom_cuda_kernel.cu -o build/temp.linux-x86_64-cpython-310/csrc/common/custom_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cpu_adam_op -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "    creating build/lib.linux-x86_64-cpython-310\n",
            "    creating build/lib.linux-x86_64-cpython-310/deepspeed\n",
            "    creating build/lib.linux-x86_64-cpython-310/deepspeed/ops\n",
            "    creating build/lib.linux-x86_64-cpython-310/deepspeed/ops/adam\n",
            "    x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/csrc/adam/cpu_adam.o build/temp.linux-x86_64-cpython-310/csrc/adam/cpu_adam_impl.o build/temp.linux-x86_64-cpython-310/csrc/common/custom_cuda_kernel.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lcurand -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/deepspeed/ops/adam/cpu_adam_op.cpython-310-x86_64-linux-gnu.so -lcurand\n",
            "    running develop\n",
            "    /usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` and ``easy_install``.\n",
            "            Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "            other standards-based tools.\n",
            "\n",
            "            See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      easy_install.initialize_options(self)\n",
            "    /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "    !!\n",
            "\n",
            "            ********************************************************************************\n",
            "            Please avoid running ``setup.py`` directly.\n",
            "            Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "            other standards-based tools.\n",
            "\n",
            "            See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "            ********************************************************************************\n",
            "\n",
            "    !!\n",
            "      self.initialize_options()\n",
            "    running egg_info\n",
            "    creating deepspeed.egg-info\n",
            "    writing deepspeed.egg-info/PKG-INFO\n",
            "    writing dependency_links to deepspeed.egg-info/dependency_links.txt\n",
            "    writing entry points to deepspeed.egg-info/entry_points.txt\n",
            "    writing requirements to deepspeed.egg-info/requires.txt\n",
            "    writing top-level names to deepspeed.egg-info/top_level.txt\n",
            "    writing manifest file 'deepspeed.egg-info/SOURCES.txt'\n",
            "    reading manifest file 'deepspeed.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    warning: no files found matching 'deepspeed/inference/v2/kernels/ragged_ops/libs/*.so'\n",
            "    warning: no files found matching 'deepspeed/inference/v2/kernels/cutlass_ops/libs/*.so'\n",
            "    warning: no files found matching '*.hip' under directory 'deepspeed'\n",
            "    warning: no files found matching '*.cc' under directory 'deepspeed'\n",
            "    warning: no files found matching '*.tr' under directory 'csrc'\n",
            "    warning: no files found matching '*.cc' under directory 'csrc'\n",
            "    warning: no files found matching '*.py' under directory 'benchmarks'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'deepspeed.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "      warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "    copying build/lib.linux-x86_64-cpython-310/deepspeed/ops/adam/cpu_adam_op.cpython-310-x86_64-linux-gnu.so -> deepspeed/ops/adam\n",
            "    Creating /usr/local/lib/python3.10/dist-packages/deepspeed.egg-link (link to .)\n",
            "    Adding deepspeed 0.13.2+18179807 to easy-install.pth file\n",
            "    Installing deepspeed script to /usr/local/bin\n",
            "    Installing deepspeed.pt script to /usr/local/bin\n",
            "    Installing ds script to /usr/local/bin\n",
            "    Installing ds_ssh script to /usr/local/bin\n",
            "    Installing ds_report script to /usr/local/bin\n",
            "    Installing ds_bench script to /usr/local/bin\n",
            "    Installing dsr script to /usr/local/bin\n",
            "    Installing ds_elastic script to /usr/local/bin\n",
            "\n",
            "    Installed /content/DeepSpeed\n",
            "    deepspeed build time = 73.97673892974854 secs\n",
            "Successfully installed deepspeed-0.13.2+18179807 hjson-3.1.0 pynvml-11.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "pip install -e .\n",
        "pip install -r examples/pytorch/translation/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXvrmaMR70i6",
        "outputId": "bcd0a240-7b50-4f51-a824-28b4eb82ea04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Checking if build backend supports build_editable: started\n",
            "  Checking if build backend supports build_editable: finished with status 'done'\n",
            "  Getting requirements to build editable: started\n",
            "  Getting requirements to build editable: finished with status 'done'\n",
            "  Preparing editable metadata (pyproject.toml): started\n",
            "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building editable for transformers (pyproject.toml): started\n",
            "  Building editable for transformers (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for transformers: filename=transformers-4.38.0.dev0-0.editable-py3-none-any.whl size=41785 sha256=d2c503075d6ad3300a366c2d3f3850191cedea581570727e8116cf5c7e77f27b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d2n645ct/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.0.dev0\n",
            "    Uninstalling transformers-4.38.0.dev0:\n",
            "      Successfully uninstalled transformers-4.38.0.dev0\n",
            "Successfully installed transformers-4.38.0.dev0\n",
            "Collecting accelerate>=0.12.0 (from -r examples/pytorch/translation/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 279.7/279.7 kB 6.1 MB/s eta 0:00:00\n",
            "Collecting datasets>=1.8.0 (from -r examples/pytorch/translation/requirements.txt (line 2))\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.6/536.6 kB 25.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 4)) (3.20.3)\n",
            "Collecting sacrebleu>=1.4.12 (from -r examples/pytorch/translation/requirements.txt (line 5))\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.3/106.3 kB 18.0 MB/s eta 0:00:00\n",
            "Collecting py7zr (from -r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 11.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 7)) (2.1.0+cu121)\n",
            "Collecting evaluate (from -r examples/pytorch/translation/requirements.txt (line 8))\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 14.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r examples/pytorch/translation/requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.13.1)\n",
            "Collecting pyarrow>=12.0.0 (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2))\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 20.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 18.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 19.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.9.3)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5)) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5)) (4.9.4)\n",
            "Collecting texttable (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 91.5 MB/s eta 0:00:00\n",
            "Collecting pyzstd>=0.15.9 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 412.3/412.3 kB 47.9 MB/s eta 0:00:00\n",
            "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.9/138.9 kB 21.8 MB/s eta 0:00:00\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.7/49.7 kB 7.7 MB/s eta 0:00:00\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.1/93.1 kB 13.6 MB/s eta 0:00:00\n",
            "Collecting brotli>=1.1.0 (from py7zr->-r examples/pytorch/translation/requirements.txt (line 6))\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 96.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (2.1.0)\n",
            "Collecting responses<0.19 (from evaluate->-r examples/pytorch/translation/requirements.txt (line 8))\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, pyarrow, portalocker, multivolumefile, inflate64, dill, colorama, sacrebleu, responses, py7zr, multiprocess, accelerate, datasets, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "Successfully installed accelerate-0.27.0 brotli-1.1.0 colorama-0.4.6 datasets-2.17.0 dill-0.3.8 evaluate-0.4.1 inflate64-1.0.0 multiprocess-0.70.16 multivolumefile-0.2.3 portalocker-2.8.2 py7zr-0.20.8 pyarrow-15.0.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.9 responses-0.18.0 sacrebleu-2.4.0 texttable-1.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'transformers'...\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd1psR0KJVMD",
        "outputId": "0d9b65cd-b258-49bf-ea27-b4646ce84400"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "# DeepSpeed ZeRO-2 模式单 GPU 训练翻译模型（T5-Small）\n",
        "cd transformers; export BS=16; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero2.json \\\n",
        "--model_name_or_path t5-small --per_device_train_batch_size 1 \\\n",
        "--output_dir output_dir --overwrite_output_dir --fp16 \\\n",
        "--do_train --max_train_samples 500 --num_train_epochs 1 \\\n",
        "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
        "--source_lang en --target_lang ro\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAH0PEOPuEJc",
        "outputId": "234226b3-61c5-4b43-8441-95e9cd9e8d80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-02-12 01:47:05,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:47:06,352] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "Detected CUDA_VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
            "[2024-02-12 01:47:06,353] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None examples/pytorch/translation/run_translation.py --deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero2.json --model_name_or_path t5-small --per_device_train_batch_size 1 --output_dir output_dir --overwrite_output_dir --fp16 --do_train --max_train_samples 500 --num_train_epochs 1 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro\n",
            "[2024-02-12 01:47:09,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.3-1\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2024-02-12 01:47:10,965] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2024-02-12 01:47:10,966] [INFO] [launch.py:253:main] process 15805 spawned with command: ['/usr/bin/python3', '-u', 'examples/pytorch/translation/run_translation.py', '--local_rank=0', '--deepspeed', '/content/drive/MyDrive/data/deepspeed/config/ds_config_zero2.json', '--model_name_or_path', 't5-small', '--per_device_train_batch_size', '1', '--output_dir', 'output_dir', '--overwrite_output_dir', '--fp16', '--do_train', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro']\n",
            "[2024-02-12 01:47:15,962] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:47:16,208] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-02-12 01:47:16,208] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "02/12/2024 01:47:18 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
            "02/12/2024 01:47:18 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=/content/drive/MyDrive/data/deepspeed/config/ds_config_zero2.json,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_dir/runs/Feb12_01-47-15_f39fabd730fe,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=output_dir,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "02/12/2024 01:47:18 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
            "02/12/2024 01:47:23 - INFO - datasets.builder - Generating dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "02/12/2024 01:47:23 - INFO - datasets.builder - Downloading and preparing dataset wmt16/ro-en (download: 274.05 MiB, generated: 180.62 MiB, post-processed: Unknown size, total: 454.67 MiB) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea...\n",
            "02/12/2024 01:47:25 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "02/12/2024 01:47:25 - INFO - datasets.utils.file_utils - hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/train/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37.incomplete\n",
            "02/12/2024 01:47:30 - INFO - datasets.utils.file_utils - storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/train/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37\n",
            "02/12/2024 01:47:30 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37\n",
            "02/12/2024 01:47:30 - INFO - datasets.utils.file_utils - hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/validation/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b.incomplete\n",
            "02/12/2024 01:47:31 - INFO - datasets.utils.file_utils - storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/validation/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b\n",
            "02/12/2024 01:47:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b\n",
            "02/12/2024 01:47:31 - INFO - datasets.utils.file_utils - hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/test/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746.incomplete\n",
            "02/12/2024 01:47:32 - INFO - datasets.utils.file_utils - storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/test/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746\n",
            "02/12/2024 01:47:32 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746\n",
            "02/12/2024 01:47:32 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "02/12/2024 01:47:32 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "02/12/2024 01:47:32 - INFO - datasets.builder - Generating train split\n",
            "02/12/2024 01:47:33 - INFO - datasets.builder - Generating validation split\n",
            "02/12/2024 01:47:33 - INFO - datasets.builder - Generating test split\n",
            "02/12/2024 01:47:33 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "02/12/2024 01:47:33 - INFO - datasets.builder - Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea. Subsequent calls will reuse this data.\n",
            "02/12/2024 01:47:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-367493374fa8637f.arrow\n",
            "[2024-02-12 01:47:42,988] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.2+18179807, git-hash=18179807, git-branch=master\n",
            "[2024-02-12 01:47:44,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
            "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2024-02-12 01:47:46,157] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2024-02-12 01:47:46,157] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-02-12 01:47:46,161] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2024-02-12 01:47:46,161] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2024-02-12 01:47:46,161] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
            "[2024-02-12 01:47:46,161] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
            "[2024-02-12 01:47:46,161] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
            "[2024-02-12 01:47:46,161] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True\n",
            "[2024-02-12 01:47:46,161] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
            "[2024-02-12 01:47:46,723] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
            "[2024-02-12 01:47:46,724] [INFO] [utils.py:801:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.15 GB         Max_CA 0 GB \n",
            "[2024-02-12 01:47:46,724] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 4.9 GB, percent = 5.9%\n",
            "[2024-02-12 01:47:47,117] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
            "[2024-02-12 01:47:47,117] [INFO] [utils.py:801:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.15 GB         Max_CA 0 GB \n",
            "[2024-02-12 01:47:47,118] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 5.16 GB, percent = 6.2%\n",
            "[2024-02-12 01:47:47,118] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
            "[2024-02-12 01:47:47,325] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-02-12 01:47:47,326] [INFO] [utils.py:801:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.15 GB         Max_CA 0 GB \n",
            "[2024-02-12 01:47:47,326] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 5.16 GB, percent = 6.2%\n",
            "[2024-02-12 01:47:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2024-02-12 01:47:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2024-02-12 01:47:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7dde82169d50>\n",
            "[2024-02-12 01:47:47,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]\n",
            "[2024-02-12 01:47:47,333] [INFO] [config.py:987:print] DeepSpeedEngine configuration:\n",
            "[2024-02-12 01:47:47,333] [INFO] [config.py:991:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   amp_enabled .................. False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   amp_params ................... False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   bfloat16_enabled ............. False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7dddfd00a8f0>\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   communication_data_type ...... None\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   curriculum_enabled_legacy .... False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   curriculum_params_legacy ..... False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   data_efficiency_enabled ...... False\n",
            "[2024-02-12 01:47:47,334] [INFO] [config.py:991:print]   dataloader_drop_last ......... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   disable_allgather ............ False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   dump_state ................... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_enabled ........... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   eigenvalue_verbose ........... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   elasticity_enabled ........... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   fp16_auto_cast ............... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   fp16_enabled ................. True\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   global_rank .................. 0\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   grad_accum_dtype ............. None\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   gradient_accumulation_steps .. 1\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   gradient_clipping ............ 1.0\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   graph_harvesting ............. False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   initial_dynamic_scale ........ 65536\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   load_universal_checkpoint .... False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   loss_scale ................... 0\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   memory_breakdown ............. False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   mics_hierarchial_params_gather  False\n",
            "[2024-02-12 01:47:47,335] [INFO] [config.py:991:print]   mics_shard_size .............. -1\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   optimizer_name ............... adamw\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   pld_enabled .................. False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   pld_params ................... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   prescale_gradients ........... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   scheduler_name ............... WarmupLR\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   sparse_attention ............. None\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   sparse_gradients_enabled ..... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   steps_per_print .............. inf\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   train_batch_size ............. 1\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   train_micro_batch_size_per_gpu  1\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   use_data_before_expert_parallel_  False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   use_node_local_storage ....... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   wall_clock_breakdown ......... False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   weight_quantization_config ... None\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   world_size ................... 1\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   zero_allow_untested_optimizer  False\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   zero_enabled ................. True\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-02-12 01:47:47,336] [INFO] [config.py:991:print]   zero_optimization_stage ...... 2\n",
            "[2024-02-12 01:47:47,337] [INFO] [config.py:977:print_user_config]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 5e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 5e-05, \n",
            "            \"warmup_num_steps\": 0\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"train_batch_size\": 1, \n",
            "    \"train_micro_batch_size_per_gpu\": 1, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "[2024-02-12 01:47:49,247] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
            "[2024-02-12 01:47:49,376] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
            "[2024-02-12 01:47:49,482] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
            "[2024-02-12 01:47:49,604] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
            "[2024-02-12 01:47:49,706] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
            "[2024-02-12 01:47:50,095] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
            "[2024-02-12 01:47:50,207] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
            "[2024-02-12 01:47:50,716] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
            "[2024-02-12 01:47:54,061] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
            "{'loss': 0.9465, 'learning_rate': 5e-05, 'epoch': 1.0}\n",
            "[2024-02-12 01:49:23,172] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!\n",
            "[2024-02-12 01:49:23,181] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: output_dir/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt\n",
            "[2024-02-12 01:49:23,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_dir/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt...\n",
            "[2024-02-12 01:49:23,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_dir/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt.\n",
            "[2024-02-12 01:49:23,416] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output_dir/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
            "[2024-02-12 01:49:24,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output_dir/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
            "[2024-02-12 01:49:24,231] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved output_dir/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
            "[2024-02-12 01:49:24,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!\n",
            "{'train_runtime': 98.3115, 'train_samples_per_second': 5.086, 'train_steps_per_second': 5.086, 'train_loss': 0.9465027465820313, 'epoch': 1.0}\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     0.9465\n",
            "  train_runtime            = 0:01:38.31\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      5.086\n",
            "  train_steps_per_second   =      5.086\n",
            "[2024-02-12 01:49:29,108] [INFO] [launch.py:348:main] Process 15805 exits successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "Downloading and preparing dataset wmt16/ro-en (download: 274.05 MiB, generated: 180.62 MiB, post-processed: Unknown size, total: 454.67 MiB) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/train/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37.incomplete\n",
            "\rDownloading data:   0%|          | 0.00/108M [00:00<?, ?B/s]\rDownloading data:   4%|▍         | 4.19M/108M [00:00<00:09, 10.8MB/s]\rDownloading data:  12%|█▏        | 12.6M/108M [00:00<00:04, 19.4MB/s]\rDownloading data:  19%|█▉        | 21.0M/108M [00:01<00:03, 22.8MB/s]\rDownloading data:  27%|██▋       | 29.4M/108M [00:01<00:03, 24.8MB/s]\rDownloading data:  35%|███▍      | 37.7M/108M [00:01<00:02, 26.2MB/s]\rDownloading data:  43%|████▎     | 46.1M/108M [00:01<00:02, 26.7MB/s]\rDownloading data:  51%|█████     | 54.5M/108M [00:02<00:01, 27.1MB/s]\rDownloading data:  58%|█████▊    | 62.9M/108M [00:02<00:01, 27.6MB/s]\rDownloading data:  66%|██████▌   | 71.3M/108M [00:02<00:01, 28.0MB/s]\rDownloading data:  74%|███████▍  | 79.7M/108M [00:03<00:01, 28.0MB/s]\rDownloading data:  82%|████████▏ | 88.1M/108M [00:03<00:00, 28.3MB/s]\rDownloading data:  89%|████████▉ | 96.5M/108M [00:03<00:00, 28.4MB/s]\rDownloading data:  97%|█████████▋| 105M/108M [00:03<00:00, 28.4MB/s] \rDownloading data: 100%|██████████| 108M/108M [00:04<00:00, 26.6MB/s]\n",
            "storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/train/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/e282454aa624184d75e04f8510187ad9e2b5b1823b80afc6262a115f3dfb4c37\n",
            "hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/validation/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b.incomplete\n",
            "\rDownloading data:   0%|          | 0.00/362k [00:00<?, ?B/s]\rDownloading data: 100%|██████████| 362k/362k [00:00<00:00, 1.30MB/s]\rDownloading data: 100%|██████████| 362k/362k [00:00<00:00, 1.30MB/s]\n",
            "storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/validation/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/62ec2bad174772b3f818c3a070b6ceb3578cbca282bbb039b440211d6d79879b\n",
            "hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/test/0000.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746.incomplete\n",
            "\rDownloading data:   0%|          | 0.00/342k [00:00<?, ?B/s]\rDownloading data: 100%|██████████| 342k/342k [00:00<00:00, 1.29MB/s]\rDownloading data: 100%|██████████| 342k/342k [00:00<00:00, 1.28MB/s]\n",
            "storing hf://datasets/wmt16@7a79a3a94787a26ef461ae44b79cfd2ca2b27aef/ro-en/test/0000.parquet in cache at /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/25d59931f6493dc5640c92cc6c9f65f74b60b5674e0f4e6a23d76d5b494cb746\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "\rGenerating train split:   0%|          | 0/610320 [00:00<?, ? examples/s]\rGenerating train split:  13%|█▎        | 80000/610320 [00:00<00:00, 750755.85 examples/s]\rGenerating train split:  29%|██▉       | 180000/610320 [00:00<00:00, 817502.60 examples/s]\rGenerating train split:  46%|████▌     | 280000/610320 [00:00<00:00, 835365.30 examples/s]\rGenerating train split:  62%|██████▏   | 380000/610320 [00:00<00:00, 845337.82 examples/s]\rGenerating train split:  79%|███████▊  | 480000/610320 [00:00<00:00, 857650.40 examples/s]\rGenerating train split:  95%|█████████▌| 580000/610320 [00:00<00:00, 865992.78 examples/s]\rGenerating train split: 100%|██████████| 610320/610320 [00:00<00:00, 844960.71 examples/s]\n",
            "Generating validation split\n",
            "\rGenerating validation split:   0%|          | 0/1999 [00:00<?, ? examples/s]\rGenerating validation split: 100%|██████████| 1999/1999 [00:00<00:00, 472974.20 examples/s]\n",
            "Generating test split\n",
            "\rGenerating test split:   0%|          | 0/1999 [00:00<?, ? examples/s]\rGenerating test split: 100%|██████████| 1999/1999 [00:00<00:00, 577677.67 examples/s]\n",
            "All the splits matched successfully.\n",
            "Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea. Subsequent calls will reuse this data.\n",
            "\rconfig.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]\rconfig.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 6.78MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 01:47:33,807 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 01:47:33,817 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "\rtokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]\rtokenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<00:00, 15.0MB/s]\n",
            "\rspiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\rspiece.model: 100%|██████████| 792k/792k [00:00<00:00, 28.4MB/s]\n",
            "\rtokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]\rtokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 5.79MB/s]\rtokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 5.75MB/s]\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:47:36,279 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:47:36,279 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:47:36,279 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:47:36,279 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:47:36,279 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
            "\rmodel.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]\rmodel.safetensors:  17%|█▋        | 41.9M/242M [00:00<00:00, 400MB/s]\rmodel.safetensors:  39%|███▉      | 94.4M/242M [00:00<00:00, 467MB/s]\rmodel.safetensors:  65%|██████▍   | 157M/242M [00:00<00:00, 499MB/s] \rmodel.safetensors:  87%|████████▋ | 210M/242M [00:00<00:00, 503MB/s]\rmodel.safetensors: 100%|██████████| 242M/242M [00:00<00:00, 482MB/s]\n",
            "[INFO|modeling_utils.py:3259] 2024-02-12 01:47:37,221 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
            "[INFO|configuration_utils.py:840] 2024-02-12 01:47:37,268 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3992] 2024-02-12 01:47:37,499 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4000] 2024-02-12 01:47:37,499 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "\rgeneration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]\rgeneration_config.json: 100%|██████████| 147/147 [00:00<00:00, 830kB/s]\n",
            "[INFO|configuration_utils.py:795] 2024-02-12 01:47:38,184 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
            "[INFO|configuration_utils.py:840] 2024-02-12 01:47:38,185 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "\rRunning tokenizer on train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-367493374fa8637f.arrow\n",
            "\rRunning tokenizer on train dataset: 100%|██████████| 500/500 [00:00<00:00, 16430.47 examples/s]\n",
            "\rDownloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]\rDownloading builder script: 100%|██████████| 8.15k/8.15k [00:00<00:00, 25.9MB/s]\n",
            "2024-02-12 01:47:41.074628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-12 01:47:41.074676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-12 01:47:41.076243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-12 01:47:42.122504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|trainer.py:586] 2024-02-12 01:47:42,768 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1747] 2024-02-12 01:47:47,337 >> ***** Running training *****\n",
            "[INFO|trainer.py:1748] 2024-02-12 01:47:47,337 >>   Num examples = 500\n",
            "[INFO|trainer.py:1749] 2024-02-12 01:47:47,337 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1750] 2024-02-12 01:47:47,337 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1753] 2024-02-12 01:47:47,337 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1754] 2024-02-12 01:47:47,337 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1755] 2024-02-12 01:47:47,337 >>   Total optimization steps = 500\n",
            "[INFO|trainer.py:1756] 2024-02-12 01:47:47,338 >>   Number of trainable parameters = 60,506,624\n",
            "\r  0%|          | 0/500 [00:00<?, ?it/s]/content/DeepSpeed/deepspeed/runtime/zero/stage_1_and_2.py:1990: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n",
            "\r  0%|          | 1/500 [00:01<15:52,  1.91s/it]\r  0%|          | 2/500 [00:02<07:09,  1.16it/s]\r  1%|          | 3/500 [00:02<04:16,  1.94it/s]\r  1%|          | 4/500 [00:02<02:58,  2.77it/s]\r  1%|          | 5/500 [00:02<02:12,  3.74it/s]\r  1%|          | 6/500 [00:02<02:14,  3.67it/s]\r  1%|▏         | 7/500 [00:02<01:47,  4.58it/s]\r  2%|▏         | 8/500 [00:02<01:30,  5.42it/s]\r  2%|▏         | 9/500 [00:03<01:33,  5.27it/s]\r  2%|▏         | 10/500 [00:03<01:35,  5.15it/s]\r  2%|▏         | 11/500 [00:03<01:21,  6.01it/s]\r  2%|▏         | 12/500 [00:03<01:25,  5.70it/s]\r  3%|▎         | 13/500 [00:03<01:28,  5.49it/s]\r  3%|▎         | 14/500 [00:03<01:31,  5.30it/s]\r  3%|▎         | 15/500 [00:04<01:33,  5.18it/s]\r  3%|▎         | 16/500 [00:04<01:36,  5.00it/s]\r  3%|▎         | 17/500 [00:04<01:36,  4.99it/s]\r  4%|▎         | 18/500 [00:04<01:36,  5.01it/s]\r  4%|▍         | 19/500 [00:04<01:33,  5.14it/s]\r  4%|▍         | 20/500 [00:05<01:31,  5.24it/s]\r  4%|▍         | 21/500 [00:05<01:30,  5.31it/s]\r  4%|▍         | 22/500 [00:05<01:28,  5.37it/s]\r  5%|▍         | 23/500 [00:05<01:27,  5.42it/s]\r  5%|▍         | 24/500 [00:05<01:27,  5.44it/s]\r  5%|▌         | 25/500 [00:06<01:27,  5.40it/s]\r  5%|▌         | 26/500 [00:06<01:27,  5.42it/s]\r  5%|▌         | 27/500 [00:06<01:26,  5.45it/s]\r  6%|▌         | 28/500 [00:06<01:26,  5.47it/s]\r  6%|▌         | 29/500 [00:06<01:15,  6.28it/s]\r  6%|▌         | 30/500 [00:06<01:17,  6.04it/s]\r  6%|▌         | 31/500 [00:07<01:20,  5.79it/s]\r  6%|▋         | 32/500 [00:07<01:23,  5.60it/s]\r  7%|▋         | 33/500 [00:07<01:25,  5.47it/s]\r  7%|▋         | 34/500 [00:07<01:24,  5.49it/s]\r  7%|▋         | 35/500 [00:07<01:24,  5.49it/s]\r  7%|▋         | 36/500 [00:08<01:24,  5.47it/s]\r  7%|▋         | 37/500 [00:08<01:25,  5.42it/s]\r  8%|▊         | 38/500 [00:08<01:26,  5.35it/s]\r  8%|▊         | 39/500 [00:08<01:25,  5.40it/s]\r  8%|▊         | 40/500 [00:08<01:24,  5.44it/s]\r  8%|▊         | 41/500 [00:08<01:23,  5.47it/s]\r  8%|▊         | 42/500 [00:09<01:24,  5.39it/s]\r  9%|▊         | 43/500 [00:09<01:24,  5.42it/s]\r  9%|▉         | 44/500 [00:09<01:23,  5.45it/s]\r  9%|▉         | 45/500 [00:09<01:23,  5.48it/s]\r  9%|▉         | 46/500 [00:09<01:22,  5.49it/s]\r  9%|▉         | 47/500 [00:10<01:22,  5.46it/s]\r 10%|▉         | 48/500 [00:10<01:23,  5.42it/s]\r 10%|▉         | 49/500 [00:10<01:23,  5.43it/s]\r 10%|█         | 50/500 [00:10<01:23,  5.37it/s]\r 10%|█         | 51/500 [00:10<01:22,  5.42it/s]\r 10%|█         | 52/500 [00:10<01:22,  5.44it/s]\r 11%|█         | 53/500 [00:11<01:22,  5.44it/s]\r 11%|█         | 54/500 [00:11<01:21,  5.45it/s]\r 11%|█         | 55/500 [00:11<01:21,  5.47it/s]\r 11%|█         | 56/500 [00:11<01:21,  5.47it/s]\r 11%|█▏        | 57/500 [00:11<01:20,  5.49it/s]\r 12%|█▏        | 58/500 [00:12<01:20,  5.50it/s]\r 12%|█▏        | 59/500 [00:12<01:21,  5.42it/s]\r 12%|█▏        | 60/500 [00:12<01:21,  5.42it/s]\r 12%|█▏        | 61/500 [00:12<01:20,  5.45it/s]\r 12%|█▏        | 62/500 [00:12<01:20,  5.47it/s]\r 13%|█▎        | 63/500 [00:12<01:19,  5.49it/s]\r 13%|█▎        | 64/500 [00:13<01:20,  5.40it/s]\r 13%|█▎        | 65/500 [00:13<01:20,  5.39it/s]\r 13%|█▎        | 66/500 [00:13<01:20,  5.42it/s]\r 13%|█▎        | 67/500 [00:13<01:19,  5.45it/s]\r 14%|█▎        | 68/500 [00:13<01:18,  5.48it/s]\r 14%|█▍        | 69/500 [00:14<01:19,  5.42it/s]\r 14%|█▍        | 70/500 [00:14<01:19,  5.42it/s]\r 14%|█▍        | 71/500 [00:14<01:18,  5.44it/s]\r 14%|█▍        | 72/500 [00:14<01:21,  5.28it/s]\r 15%|█▍        | 73/500 [00:14<01:21,  5.21it/s]\r 15%|█▍        | 74/500 [00:15<01:22,  5.14it/s]\r 15%|█▌        | 75/500 [00:15<01:25,  4.98it/s]\r 15%|█▌        | 76/500 [00:15<01:25,  4.98it/s]\r 15%|█▌        | 77/500 [00:15<01:25,  4.94it/s]\r 16%|█▌        | 78/500 [00:15<01:26,  4.89it/s]\r 16%|█▌        | 79/500 [00:16<01:26,  4.86it/s]\r 16%|█▌        | 80/500 [00:16<01:26,  4.87it/s]\r 16%|█▌        | 81/500 [00:16<01:25,  4.89it/s]\r 16%|█▋        | 82/500 [00:16<01:24,  4.95it/s]\r 17%|█▋        | 83/500 [00:16<01:21,  5.11it/s]\r 17%|█▋        | 84/500 [00:17<01:19,  5.20it/s]\r 17%|█▋        | 85/500 [00:17<01:19,  5.23it/s]\r 17%|█▋        | 86/500 [00:17<01:17,  5.31it/s]\r 17%|█▋        | 87/500 [00:17<01:16,  5.38it/s]\r 18%|█▊        | 88/500 [00:17<01:16,  5.41it/s]\r 18%|█▊        | 89/500 [00:17<01:15,  5.44it/s]\r 18%|█▊        | 90/500 [00:18<01:16,  5.33it/s]\r 18%|█▊        | 91/500 [00:18<01:15,  5.39it/s]\r 18%|█▊        | 92/500 [00:18<01:15,  5.42it/s]\r 19%|█▊        | 93/500 [00:18<01:14,  5.45it/s]\r 19%|█▉        | 94/500 [00:18<01:14,  5.47it/s]\r 19%|█▉        | 95/500 [00:19<01:14,  5.44it/s]\r 19%|█▉        | 96/500 [00:19<01:15,  5.37it/s]\r 19%|█▉        | 97/500 [00:19<01:14,  5.39it/s]\r 20%|█▉        | 98/500 [00:19<01:14,  5.43it/s]\r 20%|█▉        | 99/500 [00:19<01:13,  5.46it/s]\r 20%|██        | 100/500 [00:20<01:13,  5.47it/s]\r 20%|██        | 101/500 [00:20<01:13,  5.46it/s]\r 20%|██        | 102/500 [00:20<01:12,  5.47it/s]\r 21%|██        | 103/500 [00:20<01:12,  5.44it/s]\r 21%|██        | 104/500 [00:20<01:12,  5.47it/s]\r 21%|██        | 105/500 [00:20<01:12,  5.44it/s]\r 21%|██        | 106/500 [00:21<01:12,  5.41it/s]\r 21%|██▏       | 107/500 [00:21<01:12,  5.43it/s]\r 22%|██▏       | 108/500 [00:21<01:12,  5.38it/s]\r 22%|██▏       | 109/500 [00:21<01:12,  5.41it/s]\r 22%|██▏       | 110/500 [00:21<01:12,  5.42it/s]\r 22%|██▏       | 111/500 [00:22<01:11,  5.44it/s]\r 22%|██▏       | 112/500 [00:22<01:12,  5.32it/s]\r 23%|██▎       | 113/500 [00:22<01:12,  5.36it/s]\r 23%|██▎       | 114/500 [00:22<01:11,  5.39it/s]\r 23%|██▎       | 115/500 [00:22<01:10,  5.44it/s]\r 23%|██▎       | 116/500 [00:22<01:10,  5.44it/s]\r 23%|██▎       | 117/500 [00:23<01:10,  5.44it/s]\r 24%|██▎       | 118/500 [00:23<01:09,  5.47it/s]\r 24%|██▍       | 119/500 [00:23<01:09,  5.48it/s]\r 24%|██▍       | 120/500 [00:23<01:09,  5.49it/s]\r 24%|██▍       | 121/500 [00:23<01:09,  5.49it/s]\r 24%|██▍       | 122/500 [00:24<01:09,  5.46it/s]\r 25%|██▍       | 123/500 [00:24<01:09,  5.46it/s]\r 25%|██▍       | 124/500 [00:24<01:11,  5.29it/s]\r 25%|██▌       | 125/500 [00:24<01:10,  5.34it/s]\r 25%|██▌       | 126/500 [00:24<01:09,  5.38it/s]\r 25%|██▌       | 127/500 [00:24<01:08,  5.42it/s]\r 26%|██▌       | 128/500 [00:25<01:08,  5.43it/s]\r 26%|██▌       | 129/500 [00:25<01:08,  5.44it/s]\r 26%|██▌       | 130/500 [00:25<01:07,  5.46it/s]\r 26%|██▌       | 131/500 [00:25<01:19,  4.65it/s]\r 26%|██▋       | 132/500 [00:26<01:15,  4.86it/s]\r 27%|██▋       | 133/500 [00:26<01:12,  5.03it/s]\r 27%|██▋       | 134/500 [00:26<01:10,  5.16it/s]\r 27%|██▋       | 135/500 [00:26<01:10,  5.16it/s]\r 27%|██▋       | 136/500 [00:26<01:09,  5.21it/s]\r 27%|██▋       | 137/500 [00:26<01:09,  5.22it/s]\r 28%|██▊       | 138/500 [00:27<01:09,  5.20it/s]\r 28%|██▊       | 139/500 [00:27<01:09,  5.23it/s]\r 28%|██▊       | 140/500 [00:27<01:10,  5.12it/s]\r 28%|██▊       | 141/500 [00:27<01:10,  5.06it/s]\r 28%|██▊       | 142/500 [00:27<01:11,  5.03it/s]\r 29%|██▊       | 143/500 [00:28<01:11,  4.96it/s]\r 29%|██▉       | 144/500 [00:28<01:12,  4.94it/s]\r 29%|██▉       | 145/500 [00:28<01:12,  4.87it/s]\r 29%|██▉       | 146/500 [00:28<01:10,  4.99it/s]\r 29%|██▉       | 147/500 [00:28<01:08,  5.13it/s]\r 30%|██▉       | 148/500 [00:29<01:08,  5.15it/s]\r 30%|██▉       | 149/500 [00:29<01:06,  5.26it/s]\r 30%|███       | 150/500 [00:29<01:05,  5.33it/s]\r 30%|███       | 151/500 [00:29<01:05,  5.32it/s]\r 30%|███       | 152/500 [00:29<01:04,  5.37it/s]\r 31%|███       | 153/500 [00:30<01:04,  5.40it/s]\r 31%|███       | 154/500 [00:30<01:04,  5.40it/s]\r 31%|███       | 155/500 [00:30<01:03,  5.43it/s]\r 31%|███       | 156/500 [00:30<01:03,  5.43it/s]\r 31%|███▏      | 157/500 [00:30<01:03,  5.42it/s]\r 32%|███▏      | 158/500 [00:30<01:03,  5.43it/s]\r 32%|███▏      | 159/500 [00:31<01:03,  5.37it/s]\r 32%|███▏      | 160/500 [00:31<01:04,  5.31it/s]\r 32%|███▏      | 161/500 [00:31<01:03,  5.32it/s]\r 32%|███▏      | 162/500 [00:31<01:03,  5.34it/s]\r 33%|███▎      | 163/500 [00:31<01:02,  5.39it/s]\r 33%|███▎      | 164/500 [00:32<01:02,  5.34it/s]\r 33%|███▎      | 165/500 [00:32<01:02,  5.36it/s]\r 33%|███▎      | 166/500 [00:32<01:01,  5.40it/s]\r 33%|███▎      | 167/500 [00:32<01:01,  5.44it/s]\r 34%|███▎      | 168/500 [00:32<01:00,  5.46it/s]\r 34%|███▍      | 169/500 [00:33<01:00,  5.47it/s]\r 34%|███▍      | 170/500 [00:33<01:00,  5.44it/s]\r 34%|███▍      | 171/500 [00:33<01:00,  5.44it/s]\r 34%|███▍      | 172/500 [00:33<00:59,  5.47it/s]\r 35%|███▍      | 173/500 [00:33<01:00,  5.40it/s]\r 35%|███▍      | 174/500 [00:33<00:59,  5.44it/s]\r 35%|███▌      | 175/500 [00:34<01:00,  5.38it/s]\r 35%|███▌      | 176/500 [00:34<01:00,  5.32it/s]\r 35%|███▌      | 177/500 [00:34<01:00,  5.37it/s]\r 36%|███▌      | 178/500 [00:34<00:59,  5.42it/s]\r 36%|███▌      | 179/500 [00:34<00:59,  5.42it/s]\r 36%|███▌      | 180/500 [00:35<00:58,  5.43it/s]\r 36%|███▌      | 181/500 [00:35<00:59,  5.38it/s]\r 36%|███▋      | 182/500 [00:35<00:58,  5.40it/s]\r 37%|███▋      | 183/500 [00:35<00:58,  5.44it/s]\r 37%|███▋      | 184/500 [00:35<00:57,  5.46it/s]\r 37%|███▋      | 185/500 [00:35<00:57,  5.46it/s]\r 37%|███▋      | 186/500 [00:36<00:58,  5.37it/s]\r 37%|███▋      | 187/500 [00:36<00:58,  5.32it/s]\r 38%|███▊      | 188/500 [00:36<00:57,  5.38it/s]\r 38%|███▊      | 189/500 [00:36<00:57,  5.43it/s]\r 38%|███▊      | 190/500 [00:36<00:56,  5.46it/s]\r 38%|███▊      | 191/500 [00:37<00:56,  5.45it/s]\r 38%|███▊      | 192/500 [00:37<00:56,  5.43it/s]\r 39%|███▊      | 193/500 [00:37<00:57,  5.33it/s]\r 39%|███▉      | 194/500 [00:37<00:57,  5.32it/s]\r 39%|███▉      | 195/500 [00:37<00:57,  5.32it/s]\r 39%|███▉      | 196/500 [00:38<00:56,  5.35it/s]\r 39%|███▉      | 197/500 [00:38<00:56,  5.33it/s]\r 40%|███▉      | 198/500 [00:38<00:56,  5.38it/s]\r 40%|███▉      | 199/500 [00:38<00:55,  5.43it/s]\r 40%|████      | 200/500 [00:38<00:55,  5.38it/s]\r 40%|████      | 201/500 [00:38<00:56,  5.26it/s]\r 40%|████      | 202/500 [00:39<00:56,  5.26it/s]\r 41%|████      | 203/500 [00:39<00:56,  5.30it/s]\r 41%|████      | 204/500 [00:39<00:56,  5.28it/s]\r 41%|████      | 205/500 [00:39<00:55,  5.29it/s]\r 41%|████      | 206/500 [00:39<00:55,  5.29it/s]\r 41%|████▏     | 207/500 [00:40<00:55,  5.27it/s]\r 42%|████▏     | 208/500 [00:40<00:55,  5.23it/s]\r 42%|████▏     | 209/500 [00:40<00:56,  5.18it/s]\r 42%|████▏     | 210/500 [00:40<00:56,  5.11it/s]\r 42%|████▏     | 211/500 [00:40<00:57,  5.04it/s]\r 42%|████▏     | 212/500 [00:41<00:55,  5.16it/s]\r 43%|████▎     | 213/500 [00:41<00:54,  5.27it/s]\r 43%|████▎     | 214/500 [00:41<00:53,  5.34it/s]\r 43%|████▎     | 215/500 [00:41<00:52,  5.38it/s]\r 43%|████▎     | 216/500 [00:41<00:52,  5.43it/s]\r 43%|████▎     | 217/500 [00:41<00:52,  5.41it/s]\r 44%|████▎     | 218/500 [00:42<00:52,  5.37it/s]\r 44%|████▍     | 219/500 [00:42<00:51,  5.41it/s]\r 44%|████▍     | 220/500 [00:42<00:51,  5.43it/s]\r 44%|████▍     | 221/500 [00:42<00:51,  5.43it/s]\r 44%|████▍     | 222/500 [00:42<00:52,  5.32it/s]\r 45%|████▍     | 223/500 [00:43<00:52,  5.30it/s]\r 45%|████▍     | 224/500 [00:43<00:52,  5.30it/s]\r 45%|████▌     | 225/500 [00:43<00:51,  5.36it/s]\r 45%|████▌     | 226/500 [00:43<00:50,  5.40it/s]\r 45%|████▌     | 227/500 [00:43<00:50,  5.43it/s]\r 46%|████▌     | 228/500 [00:44<00:50,  5.36it/s]\r 46%|████▌     | 229/500 [00:44<00:50,  5.38it/s]\r 46%|████▌     | 230/500 [00:44<00:49,  5.41it/s]\r 46%|████▌     | 231/500 [00:44<00:49,  5.44it/s]\r 46%|████▋     | 232/500 [00:44<00:49,  5.42it/s]\r 47%|████▋     | 233/500 [00:44<00:49,  5.39it/s]\r 47%|████▋     | 234/500 [00:45<00:50,  5.29it/s]\r 47%|████▋     | 235/500 [00:45<00:50,  5.27it/s]\r 47%|████▋     | 236/500 [00:45<00:49,  5.33it/s]\r 47%|████▋     | 237/500 [00:45<00:48,  5.38it/s]\r 48%|████▊     | 238/500 [00:45<00:48,  5.37it/s]\r 48%|████▊     | 239/500 [00:46<00:49,  5.24it/s]\r 48%|████▊     | 240/500 [00:46<00:49,  5.22it/s]\r 48%|████▊     | 241/500 [00:46<00:49,  5.27it/s]\r 48%|████▊     | 242/500 [00:46<00:49,  5.25it/s]\r 49%|████▊     | 243/500 [00:46<00:48,  5.32it/s]\r 49%|████▉     | 244/500 [00:47<00:47,  5.36it/s]\r 49%|████▉     | 245/500 [00:47<00:47,  5.38it/s]\r 49%|████▉     | 246/500 [00:47<00:47,  5.39it/s]\r 49%|████▉     | 247/500 [00:47<00:46,  5.41it/s]\r 50%|████▉     | 248/500 [00:47<00:46,  5.39it/s]\r 50%|████▉     | 249/500 [00:47<00:46,  5.39it/s]\r 50%|█████     | 250/500 [00:48<00:46,  5.36it/s]\r 50%|█████     | 251/500 [00:48<00:46,  5.35it/s]\r 50%|█████     | 252/500 [00:48<00:46,  5.33it/s]\r 51%|█████     | 253/500 [00:48<00:46,  5.36it/s]\r 51%|█████     | 254/500 [00:48<00:45,  5.37it/s]\r 51%|█████     | 255/500 [00:49<00:45,  5.35it/s]\r 51%|█████     | 256/500 [00:49<00:45,  5.35it/s]\r 51%|█████▏    | 257/500 [00:49<00:45,  5.39it/s]\r 52%|█████▏    | 258/500 [00:49<00:44,  5.42it/s]\r 52%|█████▏    | 259/500 [00:49<00:44,  5.39it/s]\r 52%|█████▏    | 260/500 [00:50<00:45,  5.28it/s]\r 52%|█████▏    | 261/500 [00:50<00:44,  5.32it/s]\r 52%|█████▏    | 262/500 [00:50<00:44,  5.32it/s]\r 53%|█████▎    | 263/500 [00:50<00:44,  5.37it/s]\r 53%|█████▎    | 264/500 [00:50<00:43,  5.41it/s]\r 53%|█████▎    | 265/500 [00:50<00:43,  5.35it/s]\r 53%|█████▎    | 266/500 [00:51<00:45,  5.19it/s]\r 53%|█████▎    | 267/500 [00:51<00:45,  5.12it/s]\r 54%|█████▎    | 268/500 [00:51<00:46,  5.04it/s]\r 54%|█████▍    | 269/500 [00:51<00:46,  5.01it/s]\r 54%|█████▍    | 270/500 [00:51<00:45,  5.01it/s]\r 54%|█████▍    | 271/500 [00:52<00:46,  4.98it/s]\r 54%|█████▍    | 272/500 [00:52<00:46,  4.95it/s]\r 55%|█████▍    | 273/500 [00:52<00:46,  4.90it/s]\r 55%|█████▍    | 274/500 [00:52<00:46,  4.84it/s]\r 55%|█████▌    | 275/500 [00:53<00:46,  4.84it/s]\r 55%|█████▌    | 276/500 [00:53<00:45,  4.94it/s]\r 55%|█████▌    | 277/500 [00:53<00:43,  5.09it/s]\r 56%|█████▌    | 278/500 [00:53<00:42,  5.20it/s]\r 56%|█████▌    | 279/500 [00:53<00:41,  5.28it/s]\r 56%|█████▌    | 280/500 [00:53<00:41,  5.25it/s]\r 56%|█████▌    | 281/500 [00:54<00:42,  5.18it/s]\r 56%|█████▋    | 282/500 [00:54<00:41,  5.20it/s]\r 57%|█████▋    | 283/500 [00:54<00:42,  5.13it/s]\r 57%|█████▋    | 284/500 [00:54<00:42,  5.10it/s]\r 57%|█████▋    | 285/500 [00:54<00:41,  5.21it/s]\r 57%|█████▋    | 286/500 [00:55<00:40,  5.26it/s]\r 57%|█████▋    | 287/500 [00:55<00:40,  5.32it/s]\r 58%|█████▊    | 288/500 [00:55<00:39,  5.36it/s]\r 58%|█████▊    | 289/500 [00:55<00:39,  5.40it/s]\r 58%|█████▊    | 290/500 [00:55<00:38,  5.44it/s]\r 58%|█████▊    | 291/500 [00:56<00:38,  5.46it/s]\r 58%|█████▊    | 292/500 [00:56<00:38,  5.42it/s]\r 59%|█████▊    | 293/500 [00:56<00:37,  5.45it/s]\r 59%|█████▉    | 294/500 [00:56<00:38,  5.33it/s]\r 59%|█████▉    | 295/500 [00:56<00:38,  5.30it/s]\r 59%|█████▉    | 296/500 [00:56<00:38,  5.33it/s]\r 59%|█████▉    | 297/500 [00:57<00:38,  5.23it/s]\r 60%|█████▉    | 298/500 [00:57<00:39,  5.16it/s]\r 60%|█████▉    | 299/500 [00:57<00:38,  5.18it/s]\r 60%|██████    | 300/500 [00:57<00:37,  5.29it/s]\r 60%|██████    | 301/500 [00:57<00:37,  5.36it/s]\r 60%|██████    | 302/500 [00:58<00:36,  5.39it/s]\r 61%|██████    | 303/500 [00:58<00:37,  5.31it/s]\r 61%|██████    | 304/500 [00:58<00:36,  5.35it/s]\r 61%|██████    | 305/500 [00:58<00:36,  5.38it/s]\r 61%|██████    | 306/500 [00:58<00:35,  5.41it/s]\r 61%|██████▏   | 307/500 [00:59<00:35,  5.43it/s]\r 62%|██████▏   | 308/500 [00:59<00:35,  5.44it/s]\r 62%|██████▏   | 309/500 [00:59<00:35,  5.45it/s]\r 62%|██████▏   | 310/500 [00:59<00:34,  5.45it/s]\r 62%|██████▏   | 311/500 [00:59<00:35,  5.37it/s]\r 62%|██████▏   | 312/500 [00:59<00:34,  5.38it/s]\r 63%|██████▎   | 313/500 [01:00<00:34,  5.39it/s]\r 63%|██████▎   | 314/500 [01:00<00:34,  5.39it/s]\r 63%|██████▎   | 315/500 [01:00<00:34,  5.41it/s]\r 63%|██████▎   | 316/500 [01:00<00:33,  5.42it/s]\r 63%|██████▎   | 317/500 [01:00<00:33,  5.45it/s]\r 64%|██████▎   | 318/500 [01:01<00:33,  5.41it/s]\r 64%|██████▍   | 319/500 [01:01<00:33,  5.34it/s]\r 64%|██████▍   | 320/500 [01:01<00:33,  5.34it/s]\r 64%|██████▍   | 321/500 [01:01<00:33,  5.34it/s]\r 64%|██████▍   | 322/500 [01:01<00:33,  5.26it/s]\r 65%|██████▍   | 323/500 [01:01<00:33,  5.33it/s]\r 65%|██████▍   | 324/500 [01:02<00:33,  5.33it/s]\r 65%|██████▌   | 325/500 [01:02<00:32,  5.32it/s]\r 65%|██████▌   | 326/500 [01:02<00:32,  5.31it/s]\r 65%|██████▌   | 327/500 [01:02<00:32,  5.31it/s]\r 66%|██████▌   | 328/500 [01:02<00:32,  5.32it/s]\r 66%|██████▌   | 329/500 [01:03<00:32,  5.28it/s]\r 66%|██████▌   | 330/500 [01:03<00:32,  5.29it/s]\r 66%|██████▌   | 331/500 [01:03<00:31,  5.31it/s]\r 66%|██████▋   | 332/500 [01:03<00:31,  5.30it/s]\r 67%|██████▋   | 333/500 [01:03<00:31,  5.28it/s]\r 67%|██████▋   | 334/500 [01:04<00:31,  5.28it/s]\r 67%|██████▋   | 335/500 [01:04<00:31,  5.28it/s]\r 67%|██████▋   | 336/500 [01:04<00:31,  5.22it/s]\r 67%|██████▋   | 337/500 [01:04<00:31,  5.21it/s]\r 68%|██████▊   | 338/500 [01:04<00:30,  5.24it/s]\r 68%|██████▊   | 339/500 [01:05<00:30,  5.27it/s]\r 68%|██████▊   | 340/500 [01:05<00:30,  5.31it/s]\r 68%|██████▊   | 341/500 [01:05<00:29,  5.34it/s]\r 68%|██████▊   | 342/500 [01:05<00:29,  5.35it/s]\r 69%|██████▊   | 343/500 [01:05<00:30,  5.23it/s]\r 69%|██████▉   | 344/500 [01:05<00:30,  5.20it/s]\r 69%|██████▉   | 345/500 [01:06<00:29,  5.25it/s]\r 69%|██████▉   | 346/500 [01:06<00:28,  5.33it/s]\r 69%|██████▉   | 347/500 [01:06<00:28,  5.30it/s]\r 70%|██████▉   | 348/500 [01:06<00:28,  5.36it/s]\r 70%|██████▉   | 349/500 [01:06<00:27,  5.40it/s]\r 70%|███████   | 350/500 [01:07<00:27,  5.43it/s]\r 70%|███████   | 351/500 [01:07<00:27,  5.44it/s]\r 70%|███████   | 352/500 [01:07<00:27,  5.46it/s]\r 71%|███████   | 353/500 [01:07<00:26,  5.47it/s]\r 71%|███████   | 354/500 [01:07<00:26,  5.47it/s]\r 71%|███████   | 355/500 [01:07<00:26,  5.45it/s]\r 71%|███████   | 356/500 [01:08<00:26,  5.46it/s]\r 71%|███████▏  | 357/500 [01:08<00:26,  5.48it/s]\r 72%|███████▏  | 358/500 [01:08<00:25,  5.49it/s]\r 72%|███████▏  | 359/500 [01:08<00:25,  5.46it/s]\r 72%|███████▏  | 360/500 [01:08<00:25,  5.47it/s]\r 72%|███████▏  | 361/500 [01:09<00:25,  5.42it/s]\r 72%|███████▏  | 362/500 [01:09<00:25,  5.44it/s]\r 73%|███████▎  | 363/500 [01:09<00:25,  5.46it/s]\r 73%|███████▎  | 364/500 [01:09<00:24,  5.47it/s]\r 73%|███████▎  | 365/500 [01:09<00:24,  5.46it/s]\r 73%|███████▎  | 366/500 [01:10<00:24,  5.46it/s]\r 73%|███████▎  | 367/500 [01:10<00:24,  5.47it/s]\r 74%|███████▎  | 368/500 [01:10<00:24,  5.48it/s]\r 74%|███████▍  | 369/500 [01:10<00:24,  5.30it/s]\r 74%|███████▍  | 370/500 [01:10<00:24,  5.35it/s]\r 74%|███████▍  | 371/500 [01:10<00:24,  5.27it/s]\r 74%|███████▍  | 372/500 [01:11<00:24,  5.31it/s]\r 75%|███████▍  | 373/500 [01:11<00:24,  5.27it/s]\r 75%|███████▍  | 374/500 [01:11<00:23,  5.29it/s]\r 75%|███████▌  | 375/500 [01:11<00:23,  5.35it/s]\r 75%|███████▌  | 376/500 [01:11<00:23,  5.35it/s]\r 75%|███████▌  | 377/500 [01:12<00:22,  5.40it/s]\r 76%|███████▌  | 378/500 [01:12<00:22,  5.43it/s]\r 76%|███████▌  | 379/500 [01:12<00:22,  5.44it/s]\r 76%|███████▌  | 380/500 [01:12<00:22,  5.42it/s]\r 76%|███████▌  | 381/500 [01:12<00:21,  5.43it/s]\r 76%|███████▋  | 382/500 [01:12<00:21,  5.42it/s]\r 77%|███████▋  | 383/500 [01:13<00:21,  5.42it/s]\r 77%|███████▋  | 384/500 [01:13<00:21,  5.39it/s]\r 77%|███████▋  | 385/500 [01:13<00:21,  5.29it/s]\r 77%|███████▋  | 386/500 [01:13<00:21,  5.35it/s]\r 77%|███████▋  | 387/500 [01:13<00:21,  5.37it/s]\r 78%|███████▊  | 388/500 [01:14<00:20,  5.39it/s]\r 78%|███████▊  | 389/500 [01:14<00:20,  5.36it/s]\r 78%|███████▊  | 390/500 [01:14<00:20,  5.42it/s]\r 78%|███████▊  | 391/500 [01:14<00:20,  5.42it/s]\r 78%|███████▊  | 392/500 [01:14<00:19,  5.45it/s]\r 79%|███████▊  | 393/500 [01:15<00:19,  5.41it/s]\r 79%|███████▉  | 394/500 [01:15<00:20,  5.26it/s]\r 79%|███████▉  | 395/500 [01:15<00:20,  5.18it/s]\r 79%|███████▉  | 396/500 [01:15<00:20,  5.12it/s]\r 79%|███████▉  | 397/500 [01:15<00:20,  5.05it/s]\r 80%|███████▉  | 398/500 [01:16<00:20,  5.04it/s]\r 80%|███████▉  | 399/500 [01:16<00:20,  5.03it/s]\r 80%|████████  | 400/500 [01:16<00:19,  5.00it/s]\r 80%|████████  | 401/500 [01:16<00:19,  4.98it/s]\r 80%|████████  | 402/500 [01:16<00:19,  4.96it/s]\r 81%|████████  | 403/500 [01:17<00:19,  4.95it/s]\r 81%|████████  | 404/500 [01:17<00:19,  4.97it/s]\r 81%|████████  | 405/500 [01:17<00:18,  5.06it/s]\r 81%|████████  | 406/500 [01:17<00:18,  5.16it/s]\r 81%|████████▏ | 407/500 [01:17<00:17,  5.26it/s]\r 82%|████████▏ | 408/500 [01:17<00:17,  5.24it/s]\r 82%|████████▏ | 409/500 [01:18<00:17,  5.22it/s]\r 82%|████████▏ | 410/500 [01:18<00:16,  5.31it/s]\r 82%|████████▏ | 411/500 [01:18<00:16,  5.36it/s]\r 82%|████████▏ | 412/500 [01:18<00:16,  5.40it/s]\r 83%|████████▎ | 413/500 [01:18<00:16,  5.37it/s]\r 83%|████████▎ | 414/500 [01:19<00:15,  5.39it/s]\r 83%|████████▎ | 415/500 [01:19<00:15,  5.39it/s]\r 83%|████████▎ | 416/500 [01:19<00:15,  5.41it/s]\r 83%|████████▎ | 417/500 [01:19<00:15,  5.37it/s]\r 84%|████████▎ | 418/500 [01:19<00:15,  5.36it/s]\r 84%|████████▍ | 419/500 [01:20<00:15,  5.36it/s]\r 84%|████████▍ | 420/500 [01:20<00:14,  5.39it/s]\r 84%|████████▍ | 421/500 [01:20<00:14,  5.43it/s]\r 84%|████████▍ | 422/500 [01:20<00:14,  5.45it/s]\r 85%|████████▍ | 423/500 [01:20<00:14,  5.47it/s]\r 85%|████████▍ | 424/500 [01:20<00:13,  5.49it/s]\r 85%|████████▌ | 425/500 [01:21<00:13,  5.38it/s]\r 85%|████████▌ | 426/500 [01:21<00:13,  5.42it/s]\r 85%|████████▌ | 427/500 [01:21<00:13,  5.45it/s]\r 86%|████████▌ | 428/500 [01:21<00:13,  5.45it/s]\r 86%|████████▌ | 429/500 [01:21<00:13,  5.45it/s]\r 86%|████████▌ | 430/500 [01:22<00:13,  5.36it/s]\r 86%|████████▌ | 431/500 [01:22<00:13,  5.29it/s]\r 86%|████████▋ | 432/500 [01:22<00:12,  5.24it/s]\r 87%|████████▋ | 433/500 [01:22<00:12,  5.30it/s]\r 87%|████████▋ | 434/500 [01:22<00:12,  5.33it/s]\r 87%|████████▋ | 435/500 [01:23<00:12,  5.27it/s]\r 87%|████████▋ | 436/500 [01:23<00:12,  5.25it/s]\r 87%|████████▋ | 437/500 [01:23<00:12,  5.23it/s]\r 88%|████████▊ | 438/500 [01:23<00:11,  5.17it/s]\r 88%|████████▊ | 439/500 [01:23<00:11,  5.20it/s]\r 88%|████████▊ | 440/500 [01:23<00:11,  5.29it/s]\r 88%|████████▊ | 441/500 [01:24<00:11,  5.30it/s]\r 88%|████████▊ | 442/500 [01:24<00:10,  5.36it/s]\r 89%|████████▊ | 443/500 [01:24<00:10,  5.41it/s]\r 89%|████████▉ | 444/500 [01:24<00:10,  5.43it/s]\r 89%|████████▉ | 445/500 [01:24<00:10,  5.30it/s]\r 89%|████████▉ | 446/500 [01:25<00:10,  5.35it/s]\r 89%|████████▉ | 447/500 [01:25<00:09,  5.40it/s]\r 90%|████████▉ | 448/500 [01:25<00:09,  5.42it/s]\r 90%|████████▉ | 449/500 [01:25<00:09,  5.44it/s]\r 90%|█████████ | 450/500 [01:25<00:09,  5.45it/s]\r 90%|█████████ | 451/500 [01:26<00:08,  5.45it/s]\r 90%|█████████ | 452/500 [01:26<00:08,  5.45it/s]\r 91%|█████████ | 453/500 [01:26<00:08,  5.45it/s]\r 91%|█████████ | 454/500 [01:26<00:08,  5.46it/s]\r 91%|█████████ | 455/500 [01:26<00:08,  5.47it/s]\r 91%|█████████ | 456/500 [01:26<00:08,  5.40it/s]\r 91%|█████████▏| 457/500 [01:27<00:08,  5.29it/s]\r 92%|█████████▏| 458/500 [01:27<00:07,  5.28it/s]\r 92%|█████████▏| 459/500 [01:27<00:07,  5.30it/s]\r 92%|█████████▏| 460/500 [01:27<00:07,  5.31it/s]\r 92%|█████████▏| 461/500 [01:27<00:07,  5.28it/s]\r 92%|█████████▏| 462/500 [01:28<00:07,  5.22it/s]\r 93%|█████████▎| 463/500 [01:28<00:07,  5.11it/s]\r 93%|█████████▎| 464/500 [01:28<00:07,  5.02it/s]\r 93%|█████████▎| 465/500 [01:28<00:07,  4.98it/s]\r 93%|█████████▎| 466/500 [01:28<00:06,  4.90it/s]\r 93%|█████████▎| 467/500 [01:29<00:06,  4.92it/s]\r 94%|█████████▎| 468/500 [01:29<00:06,  5.04it/s]\r 94%|█████████▍| 469/500 [01:29<00:06,  5.13it/s]\r 94%|█████████▍| 470/500 [01:29<00:05,  5.14it/s]\r 94%|█████████▍| 471/500 [01:29<00:05,  5.17it/s]\r 94%|█████████▍| 472/500 [01:30<00:05,  5.18it/s]\r 95%|█████████▍| 473/500 [01:30<00:05,  5.16it/s]\r 95%|█████████▍| 474/500 [01:30<00:05,  5.16it/s]\r 95%|█████████▌| 475/500 [01:30<00:04,  5.26it/s]\r 95%|█████████▌| 476/500 [01:30<00:04,  5.31it/s]\r 95%|█████████▌| 477/500 [01:31<00:04,  5.30it/s]\r 96%|█████████▌| 478/500 [01:31<00:04,  5.33it/s]\r 96%|█████████▌| 479/500 [01:31<00:03,  5.37it/s]\r 96%|█████████▌| 480/500 [01:31<00:03,  5.40it/s]\r 96%|█████████▌| 481/500 [01:31<00:03,  5.42it/s]\r 96%|█████████▋| 482/500 [01:31<00:03,  5.45it/s]\r 97%|█████████▋| 483/500 [01:32<00:03,  5.42it/s]\r 97%|█████████▋| 484/500 [01:32<00:02,  5.44it/s]\r 97%|█████████▋| 485/500 [01:32<00:02,  5.45it/s]\r 97%|█████████▋| 486/500 [01:32<00:02,  5.45it/s]\r 97%|█████████▋| 487/500 [01:32<00:02,  5.45it/s]\r 98%|█████████▊| 488/500 [01:33<00:02,  5.39it/s]\r 98%|█████████▊| 489/500 [01:33<00:02,  5.41it/s]\r 98%|█████████▊| 490/500 [01:33<00:01,  5.39it/s]\r 98%|█████████▊| 491/500 [01:33<00:01,  5.38it/s]\r 98%|█████████▊| 492/500 [01:33<00:01,  5.36it/s]\r 99%|█████████▊| 493/500 [01:33<00:01,  5.33it/s]\r 99%|█████████▉| 494/500 [01:34<00:01,  5.34it/s]\r 99%|█████████▉| 495/500 [01:34<00:00,  5.32it/s]\r 99%|█████████▉| 496/500 [01:34<00:00,  5.38it/s]\r 99%|█████████▉| 497/500 [01:34<00:00,  5.41it/s]\r100%|█████████▉| 498/500 [01:34<00:00,  5.39it/s]\r100%|█████████▉| 499/500 [01:35<00:00,  5.41it/s]\r100%|██████████| 500/500 [01:35<00:00,  5.44it/s]\r                                                 \r\r100%|██████████| 500/500 [01:35<00:00,  5.44it/s][INFO|trainer.py:2981] 2024-02-12 01:49:22,803 >> Saving model checkpoint to output_dir/tmp-checkpoint-500\n",
            "[INFO|configuration_utils.py:473] 2024-02-12 01:49:22,804 >> Configuration saved in output_dir/tmp-checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:608] 2024-02-12 01:49:22,805 >> Configuration saved in output_dir/tmp-checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:2454] 2024-02-12 01:49:23,139 >> Model weights saved in output_dir/tmp-checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2435] 2024-02-12 01:49:23,141 >> tokenizer config file saved in output_dir/tmp-checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2444] 2024-02-12 01:49:23,142 >> Special tokens file saved in output_dir/tmp-checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:191] 2024-02-12 01:49:23,150 >> Copy vocab file to output_dir/tmp-checkpoint-500/spiece.model\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1988] 2024-02-12 01:49:25,649 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                                 \r\r100%|██████████| 500/500 [01:38<00:00,  5.44it/s]\r100%|██████████| 500/500 [01:38<00:00,  5.09it/s]\n",
            "[INFO|trainer.py:2981] 2024-02-12 01:49:25,823 >> Saving model checkpoint to output_dir\n",
            "[INFO|configuration_utils.py:473] 2024-02-12 01:49:25,825 >> Configuration saved in output_dir/config.json\n",
            "[INFO|configuration_utils.py:608] 2024-02-12 01:49:25,825 >> Configuration saved in output_dir/generation_config.json\n",
            "[INFO|modeling_utils.py:2454] 2024-02-12 01:49:26,166 >> Model weights saved in output_dir/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2435] 2024-02-12 01:49:26,169 >> tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2444] 2024-02-12 01:49:26,169 >> Special tokens file saved in output_dir/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:191] 2024-02-12 01:49:26,170 >> Copy vocab file to output_dir/spiece.model\n",
            "[INFO|modelcard.py:452] 2024-02-12 01:49:26,780 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 ro-en', 'type': 'wmt16', 'args': 'ro-en'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSpeed ZeRO-3 模式单 GPU 训练翻译模型（T5-3B）\n",
        "%%bash\n",
        "\n",
        "cd transformers; export BS=20; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json \\\n",
        "--model_name_or_path t5-3b  \\\n",
        "--per_device_train_batch_size $BS \\\n",
        "--per_device_eval_batch_size $BS \\\n",
        "--output_dir output_dir --overwrite_output_dir \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--max_train_samples 500 --num_train_epochs 1 \\\n",
        "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
        "--source_lang en --target_lang ro \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4GEz7NbKg9J",
        "outputId": "ffe5e62b-7529-4b8d-9442-2ed7e6a4c635"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-02-12 01:50:17,006] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:50:18,222] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "Detected CUDA_VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
            "[2024-02-12 01:50:18,222] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None examples/pytorch/translation/run_translation.py --deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json --model_name_or_path t5-3b --per_device_train_batch_size 20 --per_device_eval_batch_size 20 --output_dir output_dir --overwrite_output_dir --do_train --do_eval --max_train_samples 500 --num_train_epochs 1 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --gradient_accumulation_steps 8 --fp16\n",
            "[2024-02-12 01:50:21,659] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.3-1\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2024-02-12 01:50:22,858] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2024-02-12 01:50:22,859] [INFO] [launch.py:253:main] process 16790 spawned with command: ['/usr/bin/python3', '-u', 'examples/pytorch/translation/run_translation.py', '--local_rank=0', '--deepspeed', '/content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '20', '--per_device_eval_batch_size', '20', '--output_dir', 'output_dir', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro', '--gradient_accumulation_steps', '8', '--fp16']\n",
            "[2024-02-12 01:50:27,681] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 01:50:28,069] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-02-12 01:50:28,069] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "02/12/2024 01:50:29 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
            "02/12/2024 01:50:29 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=/content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_dir/runs/Feb12_01-50-26_f39fabd730fe,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=output_dir,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=20,\n",
            "per_device_train_batch_size=20,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "02/12/2024 01:50:29 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "02/12/2024 01:50:33 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "02/12/2024 01:50:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "02/12/2024 01:50:33 - INFO - datasets.builder - Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "02/12/2024 01:50:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "config.json: 100% 1.20k/1.20k [00:00<00:00, 7.11MB/s]\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 01:50:34,234 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 01:50:34,238 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:607] 2024-02-12 01:50:34,504 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 01:50:34,766 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 01:50:34,767 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "spiece.model: 100% 792k/792k [00:00<00:00, 49.8MB/s]\n",
            "tokenizer.json: 100% 1.39M/1.39M [00:00<00:00, 2.83MB/s]\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:50:37,220 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/spiece.model\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:50:37,220 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:50:37,220 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:50:37,220 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 01:50:37,220 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 01:50:37,220 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 01:50:37,221 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "/content/transformers/src/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "model.safetensors: 100% 11.4G/11.4G [10:14<00:00, 18.5MB/s]\n",
            "[INFO|modeling_utils.py:3259] 2024-02-12 02:00:53,731 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/model.safetensors\n",
            "[INFO|modeling_utils.py:3365] 2024-02-12 02:00:53,754 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
            "[INFO|configuration_utils.py:840] 2024-02-12 02:00:53,759 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[2024-02-12 02:00:59,143] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 510, num_elems = 2.88B\n",
            "[INFO|modeling_utils.py:3992] 2024-02-12 02:01:03,058 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4000] 2024-02-12 02:01:03,059 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-3b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:3546] 2024-02-12 02:01:03,469 >> Generation config file not found, using a generation config created from the model config.\n",
            "[INFO|modeling_utils.py:1875] 2024-02-12 02:01:03,765 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Running tokenizer on train dataset:   0% 0/500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-0f02d2fa05e25b9d.arrow\n",
            "02/12/2024 02:01:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-0f02d2fa05e25b9d.arrow\n",
            "Running tokenizer on train dataset: 100% 500/500 [00:00<00:00, 17121.15 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1999 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-1732bc880d535e8b.arrow\n",
            "02/12/2024 02:01:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-1732bc880d535e8b.arrow\n",
            "Running tokenizer on validation dataset: 100% 1999/1999 [00:00<00:00, 12907.28 examples/s]\n",
            "2024-02-12 02:01:06.123465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-12 02:01:06.123518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-12 02:01:06.125285: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-12 02:01:07.190048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|trainer.py:586] 2024-02-12 02:01:07,640 >> Using auto half precision backend\n",
            "[2024-02-12 02:01:07,887] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.2+18179807, git-hash=18179807, git-branch=master\n",
            "[2024-02-12 02:01:07,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
            "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2024-02-12 02:01:09,558] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2024-02-12 02:01:09,558] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-02-12 02:01:09,607] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2024-02-12 02:01:09,607] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2024-02-12 02:01:09,608] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
            "[2024-02-12 02:01:09,608] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
            "[2024-02-12 02:01:09,830] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\n",
            "[2024-02-12 02:01:09,831] [INFO] [utils.py:801:see_memory_usage] MA 0.06 GB         Max_MA 0.18 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:09,831] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.42 GB, percent = 14.9%\n",
            "[2024-02-12 02:01:09,836] [INFO] [stage3.py:130:__init__] Reduce bucket size 1048576\n",
            "[2024-02-12 02:01:09,836] [INFO] [stage3.py:131:__init__] Prefetch bucket size 943718\n",
            "[2024-02-12 02:01:10,051] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
            "[2024-02-12 02:01:10,052] [INFO] [utils.py:801:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:10,052] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.42 GB, percent = 14.9%\n",
            "Parameter Offload: Total persistent parameters: 126976 in 124 params\n",
            "[2024-02-12 02:01:10,342] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
            "[2024-02-12 02:01:10,343] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:10,343] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.42 GB, percent = 14.9%\n",
            "[2024-02-12 02:01:10,568] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\n",
            "[2024-02-12 02:01:10,569] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:10,569] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.42 GB, percent = 14.9%\n",
            "[2024-02-12 02:01:15,534] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 3\n",
            "[2024-02-12 02:01:15,535] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:15,535] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.58 GB, percent = 24.7%\n",
            "[2024-02-12 02:01:15,759] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\n",
            "[2024-02-12 02:01:15,760] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:15,760] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.58 GB, percent = 24.7%\n",
            "[2024-02-12 02:01:18,050] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\n",
            "[2024-02-12 02:01:18,051] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:18,051] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.06 GB, percent = 39.6%\n",
            "[2024-02-12 02:01:18,284] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
            "[2024-02-12 02:01:18,285] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:18,285] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 33.06 GB, percent = 39.6%\n",
            "[2024-02-12 02:01:27,461] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
            "[2024-02-12 02:01:27,462] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:27,462] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.32 GB, percent = 54.3%\n",
            "[2024-02-12 02:01:27,463] [INFO] [stage3.py:487:_setup_for_real_optimizer] optimizer state initialized\n",
            "[2024-02-12 02:01:33,624] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-02-12 02:01:33,625] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.12 GB         CA 0.19 GB         Max_CA 0 GB \n",
            "[2024-02-12 02:01:33,625] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 53.47 GB, percent = 64.1%\n",
            "[2024-02-12 02:01:33,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2024-02-12 02:01:33,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2024-02-12 02:01:33,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x799ad70f5150>\n",
            "[2024-02-12 02:01:33,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]\n",
            "[2024-02-12 02:01:33,628] [INFO] [config.py:987:print] DeepSpeedEngine configuration:\n",
            "[2024-02-12 02:01:33,628] [INFO] [config.py:991:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-02-12 02:01:33,628] [INFO] [config.py:991:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-02-12 02:01:33,628] [INFO] [config.py:991:print]   amp_enabled .................. False\n",
            "[2024-02-12 02:01:33,628] [INFO] [config.py:991:print]   amp_params ................... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   bfloat16_enabled ............. False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x799b34842e30>\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   communication_data_type ...... None\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   curriculum_enabled_legacy .... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   curriculum_params_legacy ..... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   data_efficiency_enabled ...... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   dataloader_drop_last ......... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   disable_allgather ............ False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   dump_state ................... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_enabled ........... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   eigenvalue_verbose ........... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   elasticity_enabled ........... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   fp16_auto_cast ............... False\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   fp16_enabled ................. True\n",
            "[2024-02-12 02:01:33,629] [INFO] [config.py:991:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   global_rank .................. 0\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   grad_accum_dtype ............. None\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   gradient_accumulation_steps .. 8\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   gradient_clipping ............ 1.0\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   graph_harvesting ............. False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   initial_dynamic_scale ........ 65536\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   load_universal_checkpoint .... False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   loss_scale ................... 0\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   memory_breakdown ............. False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   mics_hierarchial_params_gather  False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   mics_shard_size .............. -1\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   optimizer_name ............... adamw\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   pld_enabled .................. False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   pld_params ................... False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   prescale_gradients ........... False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   scheduler_name ............... WarmupLR\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   sparse_attention ............. None\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   sparse_gradients_enabled ..... False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   steps_per_print .............. inf\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   train_batch_size ............. 160\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   train_micro_batch_size_per_gpu  20\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   use_data_before_expert_parallel_  False\n",
            "[2024-02-12 02:01:33,630] [INFO] [config.py:991:print]   use_node_local_storage ....... False\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   wall_clock_breakdown ......... False\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   weight_quantization_config ... None\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   world_size ................... 1\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   zero_allow_untested_optimizer  False\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1048576 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=943718 param_persistence_threshold=10240 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   zero_enabled ................. True\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:991:print]   zero_optimization_stage ...... 3\n",
            "[2024-02-12 02:01:33,631] [INFO] [config.py:977:print_user_config]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 5e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 5e-05, \n",
            "            \"warmup_num_steps\": 0\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": 1.048576e+06, \n",
            "        \"stage3_prefetch_bucket_size\": 9.437184e+05, \n",
            "        \"stage3_param_persistence_threshold\": 1.024000e+04, \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 8, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"train_batch_size\": 160, \n",
            "    \"train_micro_batch_size_per_gpu\": 20, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "[INFO|trainer.py:1747] 2024-02-12 02:01:33,631 >> ***** Running training *****\n",
            "[INFO|trainer.py:1748] 2024-02-12 02:01:33,631 >>   Num examples = 500\n",
            "[INFO|trainer.py:1749] 2024-02-12 02:01:33,631 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1750] 2024-02-12 02:01:33,631 >>   Instantaneous batch size per device = 20\n",
            "[INFO|trainer.py:1753] 2024-02-12 02:01:33,631 >>   Total train batch size (w. parallel, distributed & accumulation) = 160\n",
            "[INFO|trainer.py:1754] 2024-02-12 02:01:33,631 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1755] 2024-02-12 02:01:33,631 >>   Total optimization steps = 3\n",
            "[INFO|trainer.py:1756] 2024-02-12 02:01:33,634 >>   Number of trainable parameters = 2,851,569,664\n",
            "  0% 0/3 [00:00<?, ?it/s][2024-02-12 02:02:05,892] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
            " 33% 1/3 [00:32<01:04, 32.27s/it][2024-02-12 02:02:30,266] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
            " 67% 2/3 [00:56<00:27, 27.62s/it][2024-02-12 02:02:54,621] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
            "100% 3/3 [01:20<00:00, 26.13s/it][INFO|trainer.py:1988] 2024-02-12 02:02:54,638 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 81.0044, 'train_samples_per_second': 6.173, 'train_steps_per_second': 0.037, 'train_loss': 0.0, 'epoch': 0.96}\n",
            "100% 3/3 [01:21<00:00, 27.00s/it]\n",
            "[INFO|trainer.py:2981] 2024-02-12 02:02:58,354 >> Saving model checkpoint to output_dir\n",
            "[INFO|configuration_utils.py:473] 2024-02-12 02:02:58,355 >> Configuration saved in output_dir/config.json\n",
            "[INFO|configuration_utils.py:608] 2024-02-12 02:02:58,355 >> Configuration saved in output_dir/generation_config.json\n",
            "[INFO|modeling_utils.py:2462] 2024-02-12 02:03:34,738 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at output_dir/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2435] 2024-02-12 02:03:34,740 >> tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2444] 2024-02-12 02:03:34,740 >> Special tokens file saved in output_dir/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:191] 2024-02-12 02:03:34,758 >> Copy vocab file to output_dir/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       0.96\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:01:21.00\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      6.173\n",
            "  train_steps_per_second   =      0.037\n",
            "02/12/2024 02:03:34 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3287] 2024-02-12 02:03:34,773 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3289] 2024-02-12 02:03:34,773 >>   Num examples = 1999\n",
            "[INFO|trainer.py:3292] 2024-02-12 02:03:34,773 >>   Batch size = 20\n",
            "100% 100/100 [01:15<00:00,  1.32it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       0.96\n",
            "  eval_loss               =     3.9316\n",
            "  eval_runtime            = 0:01:16.46\n",
            "  eval_samples            =       1999\n",
            "  eval_samples_per_second =     26.142\n",
            "  eval_steps_per_second   =      1.308\n",
            "[INFO|modelcard.py:452] 2024-02-12 02:04:51,814 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 ro-en', 'type': 'wmt16', 'args': 'ro-en'}}\n",
            "[2024-02-12 02:05:01,761] [INFO] [launch.py:348:main] Process 16790 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSpeed ZeRO-3 模式单 GPU 训练翻译模型（T5-3B）\n",
        "%%bash\n",
        "\n",
        "cd transformers; export BS=20; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json \\\n",
        "--model_name_or_path t5-3b  \\\n",
        "--per_device_train_batch_size $BS \\\n",
        "--per_device_eval_batch_size $BS \\\n",
        "--output_dir /content/drive/MyDrive/models/t5-3b/ --overwrite_output_dir \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--max_train_samples 500 --num_train_epochs 1 \\\n",
        "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
        "--source_lang en --target_lang ro \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3TRZZmqTi3s",
        "outputId": "4115d7fc-7573-40e9-af75-690424192312"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-02-12 03:14:22,662] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 03:14:25,204] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "Detected CUDA_VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
            "[2024-02-12 03:14:25,204] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None examples/pytorch/translation/run_translation.py --deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json --model_name_or_path t5-3b --per_device_train_batch_size 20 --per_device_eval_batch_size 20 --output_dir /content/drive/MyDrive/models/ --overwrite_output_dir --do_train --do_eval --max_train_samples 500 --num_train_epochs 1 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --gradient_accumulation_steps 8 --fp16\n",
            "[2024-02-12 03:14:28,855] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 03:14:30,141] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.19.3-1+cuda12.2\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.19.3-1\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.19.3-1+cuda12.2\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.19.3-1\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2024-02-12 03:14:30,142] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2024-02-12 03:14:30,143] [INFO] [launch.py:253:main] process 38829 spawned with command: ['/usr/bin/python3', '-u', 'examples/pytorch/translation/run_translation.py', '--local_rank=0', '--deepspeed', '/content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json', '--model_name_or_path', 't5-3b', '--per_device_train_batch_size', '20', '--per_device_eval_batch_size', '20', '--output_dir', '/content/drive/MyDrive/models/', '--overwrite_output_dir', '--do_train', '--do_eval', '--max_train_samples', '500', '--num_train_epochs', '1', '--dataset_name', 'wmt16', '--dataset_config', 'ro-en', '--source_lang', 'en', '--target_lang', 'ro', '--gradient_accumulation_steps', '8', '--fp16']\n",
            "[2024-02-12 03:14:35,456] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-02-12 03:14:35,860] [INFO] [comm.py:637:init_distributed] cdb=None\n",
            "[2024-02-12 03:14:35,860] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "02/12/2024 03:14:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
            "02/12/2024 03:14:35 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=/content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/models/runs/Feb12_03-14-34_f39fabd730fe,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=/content/drive/MyDrive/models/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=20,\n",
            "per_device_train_batch_size=20,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/models/,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "02/12/2024 03:14:35 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
            "02/12/2024 03:14:40 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "02/12/2024 03:14:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "02/12/2024 03:14:40 - INFO - datasets.builder - Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "02/12/2024 03:14:40 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "[2024-02-12 03:14:50,615] [INFO] [partition_parameters.py:343:__exit__] finished initializing model - num_params = 510, num_elems = 2.88B\n",
            "02/12/2024 03:15:51 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-0f02d2fa05e25b9d.arrow\n",
            "02/12/2024 03:15:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-8150dda321483f1c.arrow\n",
            "[2024-02-12 03:15:59,556] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.2+18179807, git-hash=18179807, git-branch=master\n",
            "[2024-02-12 03:15:59,579] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
            "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2024-02-12 03:16:01,331] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2024-02-12 03:16:01,331] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "[2024-02-12 03:16:01,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2024-02-12 03:16:01,381] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2024-02-12 03:16:01,381] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
            "[2024-02-12 03:16:01,381] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
            "[2024-02-12 03:16:01,623] [INFO] [utils.py:800:see_memory_usage] Stage 3 initialize beginning\n",
            "[2024-02-12 03:16:01,624] [INFO] [utils.py:801:see_memory_usage] MA 0.06 GB         Max_MA 0.18 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:01,624] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.19 GB, percent = 14.6%\n",
            "[2024-02-12 03:16:01,629] [INFO] [stage3.py:130:__init__] Reduce bucket size 1048576\n",
            "[2024-02-12 03:16:01,629] [INFO] [stage3.py:131:__init__] Prefetch bucket size 943718\n",
            "[2024-02-12 03:16:01,870] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
            "[2024-02-12 03:16:01,871] [INFO] [utils.py:801:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:01,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.19 GB, percent = 14.6%\n",
            "Parameter Offload: Total persistent parameters: 126976 in 124 params\n",
            "[2024-02-12 03:16:02,182] [INFO] [utils.py:800:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
            "[2024-02-12 03:16:02,183] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.06 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:02,184] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.2 GB, percent = 14.6%\n",
            "[2024-02-12 03:16:02,450] [INFO] [utils.py:800:see_memory_usage] Before creating fp16 partitions\n",
            "[2024-02-12 03:16:02,450] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:02,451] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 12.2 GB, percent = 14.6%\n",
            "[2024-02-12 03:16:07,509] [INFO] [utils.py:800:see_memory_usage] After creating fp16 partitions: 3\n",
            "[2024-02-12 03:16:07,511] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:07,511] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.36 GB, percent = 24.4%\n",
            "[2024-02-12 03:16:07,753] [INFO] [utils.py:800:see_memory_usage] Before creating fp32 partitions\n",
            "[2024-02-12 03:16:07,754] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:07,754] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 20.36 GB, percent = 24.4%\n",
            "[2024-02-12 03:16:10,301] [INFO] [utils.py:800:see_memory_usage] After creating fp32 partitions\n",
            "[2024-02-12 03:16:10,302] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:10,302] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 32.89 GB, percent = 39.4%\n",
            "[2024-02-12 03:16:10,577] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
            "[2024-02-12 03:16:10,578] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:10,578] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 32.89 GB, percent = 39.4%\n",
            "[2024-02-12 03:16:19,708] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
            "[2024-02-12 03:16:19,709] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.06 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:19,710] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 45.12 GB, percent = 54.0%\n",
            "[2024-02-12 03:16:19,711] [INFO] [stage3.py:487:_setup_for_real_optimizer] optimizer state initialized\n",
            "[2024-02-12 03:16:25,870] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2024-02-12 03:16:25,871] [INFO] [utils.py:801:see_memory_usage] MA 0.0 GB         Max_MA 0.12 GB         CA 0.19 GB         Max_CA 0 GB \n",
            "[2024-02-12 03:16:25,871] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 53.28 GB, percent = 63.8%\n",
            "[2024-02-12 03:16:25,871] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2024-02-12 03:16:25,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2024-02-12 03:16:25,872] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7cdb9d35d180>\n",
            "[2024-02-12 03:16:25,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]\n",
            "[2024-02-12 03:16:25,874] [INFO] [config.py:987:print] DeepSpeedEngine configuration:\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   amp_enabled .................. False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   amp_params ................... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   bfloat16_enabled ............. False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   checkpoint_tag_validation_enabled  True\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   checkpoint_tag_validation_fail  False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7cdbf0a02c80>\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   communication_data_type ...... None\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   curriculum_enabled_legacy .... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   curriculum_params_legacy ..... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   data_efficiency_enabled ...... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   dataloader_drop_last ......... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   disable_allgather ............ False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   dump_state ................... False\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
            "[2024-02-12 03:16:25,875] [INFO] [config.py:991:print]   eigenvalue_enabled ........... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_layer_num ......... 0\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_max_iter .......... 100\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_stability ......... 1e-06\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_tol ............... 0.01\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   eigenvalue_verbose ........... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   elasticity_enabled ........... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   fp16_auto_cast ............... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   fp16_enabled ................. True\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   fp16_master_weights_and_gradients  False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   global_rank .................. 0\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   grad_accum_dtype ............. None\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   gradient_accumulation_steps .. 8\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   gradient_clipping ............ 1.0\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   gradient_predivide_factor .... 1.0\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   graph_harvesting ............. False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   initial_dynamic_scale ........ 65536\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   load_universal_checkpoint .... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   loss_scale ................... 0\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   memory_breakdown ............. False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   mics_hierarchial_params_gather  False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   mics_shard_size .............. -1\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   optimizer_legacy_fusion ...... False\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   optimizer_name ............... adamw\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
            "[2024-02-12 03:16:25,876] [INFO] [config.py:991:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   pld_enabled .................. False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   pld_params ................... False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   prescale_gradients ........... False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   scheduler_name ............... WarmupLR\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   sparse_attention ............. None\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   sparse_gradients_enabled ..... False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   steps_per_print .............. inf\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   train_batch_size ............. 160\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   train_micro_batch_size_per_gpu  20\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   use_data_before_expert_parallel_  False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   use_node_local_storage ....... False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   wall_clock_breakdown ......... False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   weight_quantization_config ... None\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   world_size ................... 1\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   zero_allow_untested_optimizer  False\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1048576 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=943718 param_persistence_threshold=10240 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   zero_enabled ................. True\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:991:print]   zero_optimization_stage ...... 3\n",
            "[2024-02-12 03:16:25,877] [INFO] [config.py:977:print_user_config]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 5e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 5e-05, \n",
            "            \"warmup_num_steps\": 0\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": 1.048576e+06, \n",
            "        \"stage3_prefetch_bucket_size\": 9.437184e+05, \n",
            "        \"stage3_param_persistence_threshold\": 1.024000e+04, \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 8, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"train_batch_size\": 160, \n",
            "    \"train_micro_batch_size_per_gpu\": 20, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "[2024-02-12 03:16:59,570] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
            "[2024-02-12 03:17:25,359] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
            "[2024-02-12 03:17:50,650] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
            "{'train_runtime': 84.7887, 'train_samples_per_second': 5.897, 'train_steps_per_second': 0.035, 'train_loss': 0.0, 'epoch': 0.96}\n",
            "***** train metrics *****\n",
            "  epoch                    =       0.96\n",
            "  train_loss               =        0.0\n",
            "  train_runtime            = 0:01:24.78\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      5.897\n",
            "  train_steps_per_second   =      0.035\n",
            "02/12/2024 03:18:18 - INFO - __main__ - *** Evaluate ***\n",
            "***** eval metrics *****\n",
            "  epoch                   =       0.96\n",
            "  eval_loss               =     3.9316\n",
            "  eval_runtime            = 0:01:20.47\n",
            "  eval_samples            =       1999\n",
            "  eval_samples_per_second =     24.841\n",
            "  eval_steps_per_second   =      1.243\n",
            "[2024-02-12 03:19:50,498] [INFO] [launch.py:348:main] Process 38829 exits successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "Found cached dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 03:14:42,176 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 03:14:42,182 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:607] 2024-02-12 03:14:42,443 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 03:14:42,712 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 03:14:42,713 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 03:14:43,249 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/spiece.model\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 03:14:43,249 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 03:14:43,250 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 03:14:43,250 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2029] 2024-02-12 03:14:43,250 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:729] 2024-02-12 03:14:43,250 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
            "[INFO|configuration_utils.py:792] 2024-02-12 03:14:43,251 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-3b\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 16384,\n",
            "  \"d_kv\": 128,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "/content/transformers/src/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "[INFO|modeling_utils.py:3259] 2024-02-12 03:14:43,382 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/model.safetensors\n",
            "[INFO|modeling_utils.py:3365] 2024-02-12 03:14:44,836 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
            "[INFO|configuration_utils.py:840] 2024-02-12 03:14:44,843 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3992] 2024-02-12 03:15:50,606 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4000] 2024-02-12 03:15:50,606 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-3b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:3546] 2024-02-12 03:15:50,885 >> Generation config file not found, using a generation config created from the model config.\n",
            "[INFO|modeling_utils.py:1875] 2024-02-12 03:15:51,220 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-0f02d2fa05e25b9d.arrow\n",
            "\rRunning tokenizer on validation dataset:   0%|          | 0/1999 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-8150dda321483f1c.arrow\n",
            "\rRunning tokenizer on validation dataset:  50%|█████     | 1000/1999 [00:00<00:00, 9467.76 examples/s]\rRunning tokenizer on validation dataset: 100%|██████████| 1999/1999 [00:00<00:00, 10960.38 examples/s]\n",
            "2024-02-12 03:15:55.496666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-12 03:15:55.496786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-12 03:15:55.626958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-12 03:15:58.117421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|trainer.py:586] 2024-02-12 03:15:59,297 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1747] 2024-02-12 03:16:25,878 >> ***** Running training *****\n",
            "[INFO|trainer.py:1748] 2024-02-12 03:16:25,878 >>   Num examples = 500\n",
            "[INFO|trainer.py:1749] 2024-02-12 03:16:25,878 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1750] 2024-02-12 03:16:25,878 >>   Instantaneous batch size per device = 20\n",
            "[INFO|trainer.py:1753] 2024-02-12 03:16:25,878 >>   Total train batch size (w. parallel, distributed & accumulation) = 160\n",
            "[INFO|trainer.py:1754] 2024-02-12 03:16:25,878 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1755] 2024-02-12 03:16:25,878 >>   Total optimization steps = 3\n",
            "[INFO|trainer.py:1756] 2024-02-12 03:16:25,881 >>   Number of trainable parameters = 2,851,569,664\n",
            "\r  0%|          | 0/3 [00:00<?, ?it/s]\r 33%|███▎      | 1/3 [00:33<01:07, 33.68s/it]\r 67%|██████▋   | 2/3 [00:59<00:29, 29.04s/it]\r100%|██████████| 3/3 [01:24<00:00, 27.33s/it][INFO|trainer.py:1988] 2024-02-12 03:17:50,669 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\r                                             \r\r100%|██████████| 3/3 [01:24<00:00, 27.33s/it]\r100%|██████████| 3/3 [01:24<00:00, 28.26s/it]\n",
            "[INFO|trainer.py:2981] 2024-02-12 03:17:54,575 >> Saving model checkpoint to /content/drive/MyDrive/models/\n",
            "[INFO|configuration_utils.py:473] 2024-02-12 03:17:54,586 >> Configuration saved in /content/drive/MyDrive/models/config.json\n",
            "[INFO|configuration_utils.py:608] 2024-02-12 03:17:54,592 >> Configuration saved in /content/drive/MyDrive/models/generation_config.json\n",
            "[INFO|modeling_utils.py:2462] 2024-02-12 03:18:18,519 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/models/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2435] 2024-02-12 03:18:18,525 >> tokenizer config file saved in /content/drive/MyDrive/models/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2444] 2024-02-12 03:18:18,528 >> Special tokens file saved in /content/drive/MyDrive/models/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:191] 2024-02-12 03:18:18,540 >> Copy vocab file to /content/drive/MyDrive/models/spiece.model\n",
            "[INFO|trainer.py:3287] 2024-02-12 03:18:18,578 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3289] 2024-02-12 03:18:18,578 >>   Num examples = 1999\n",
            "[INFO|trainer.py:3292] 2024-02-12 03:18:18,578 >>   Batch size = 20\n",
            "\r  0%|          | 0/100 [00:00<?, ?it/s]\r  2%|▏         | 2/100 [00:02<01:54,  1.17s/it]\r  3%|▎         | 3/100 [00:03<01:38,  1.01s/it]\r  4%|▍         | 4/100 [00:03<01:28,  1.08it/s]\r  5%|▌         | 5/100 [00:04<01:23,  1.13it/s]\r  6%|▌         | 6/100 [00:05<01:20,  1.17it/s]\r  7%|▋         | 7/100 [00:06<01:17,  1.20it/s]\r  8%|▊         | 8/100 [00:07<01:16,  1.21it/s]\r  9%|▉         | 9/100 [00:07<01:14,  1.22it/s]\r 10%|█         | 10/100 [00:08<01:12,  1.24it/s]\r 11%|█         | 11/100 [00:09<01:10,  1.25it/s]\r 12%|█▏        | 12/100 [00:10<01:09,  1.26it/s]\r 13%|█▎        | 13/100 [00:11<01:08,  1.27it/s]\r 14%|█▍        | 14/100 [00:11<01:07,  1.27it/s]\r 15%|█▌        | 15/100 [00:12<01:07,  1.26it/s]\r 16%|█▌        | 16/100 [00:13<01:07,  1.24it/s]\r 17%|█▋        | 17/100 [00:14<01:06,  1.25it/s]\r 18%|█▊        | 18/100 [00:15<01:04,  1.26it/s]\r 19%|█▉        | 19/100 [00:15<01:04,  1.26it/s]\r 20%|██        | 20/100 [00:16<01:03,  1.27it/s]\r 21%|██        | 21/100 [00:17<01:02,  1.27it/s]\r 22%|██▏       | 22/100 [00:18<01:01,  1.28it/s]\r 23%|██▎       | 23/100 [00:18<01:00,  1.28it/s]\r 24%|██▍       | 24/100 [00:19<00:59,  1.28it/s]\r 25%|██▌       | 25/100 [00:20<00:58,  1.28it/s]\r 26%|██▌       | 26/100 [00:21<00:57,  1.28it/s]\r 27%|██▋       | 27/100 [00:22<00:56,  1.28it/s]\r 28%|██▊       | 28/100 [00:22<00:56,  1.28it/s]\r 29%|██▉       | 29/100 [00:23<00:55,  1.29it/s]\r 30%|███       | 30/100 [00:24<00:54,  1.29it/s]\r 31%|███       | 31/100 [00:25<00:55,  1.25it/s]\r 32%|███▏      | 32/100 [00:26<00:54,  1.25it/s]\r 33%|███▎      | 33/100 [00:26<00:53,  1.25it/s]\r 34%|███▍      | 34/100 [00:27<00:52,  1.25it/s]\r 35%|███▌      | 35/100 [00:28<00:52,  1.25it/s]\r 36%|███▌      | 36/100 [00:29<00:51,  1.24it/s]\r 37%|███▋      | 37/100 [00:30<00:50,  1.25it/s]\r 38%|███▊      | 38/100 [00:30<00:49,  1.25it/s]\r 39%|███▉      | 39/100 [00:31<00:48,  1.25it/s]\r 40%|████      | 40/100 [00:32<00:47,  1.26it/s]\r 41%|████      | 41/100 [00:33<00:46,  1.26it/s]\r 42%|████▏     | 42/100 [00:33<00:45,  1.26it/s]\r 43%|████▎     | 43/100 [00:34<00:45,  1.27it/s]\r 44%|████▍     | 44/100 [00:35<00:43,  1.27it/s]\r 45%|████▌     | 45/100 [00:36<00:42,  1.28it/s]\r 46%|████▌     | 46/100 [00:37<00:42,  1.27it/s]\r 47%|████▋     | 47/100 [00:37<00:41,  1.26it/s]\r 48%|████▊     | 48/100 [00:38<00:40,  1.28it/s]\r 49%|████▉     | 49/100 [00:39<00:39,  1.28it/s]\r 50%|█████     | 50/100 [00:40<00:39,  1.28it/s]\r 51%|█████     | 51/100 [00:41<00:38,  1.27it/s]\r 52%|█████▏    | 52/100 [00:41<00:37,  1.28it/s]\r 53%|█████▎    | 53/100 [00:42<00:36,  1.28it/s]\r 54%|█████▍    | 54/100 [00:43<00:35,  1.28it/s]\r 55%|█████▌    | 55/100 [00:44<00:34,  1.29it/s]\r 56%|█████▌    | 56/100 [00:44<00:34,  1.28it/s]\r 57%|█████▋    | 57/100 [00:45<00:33,  1.29it/s]\r 58%|█████▊    | 58/100 [00:46<00:32,  1.29it/s]\r 59%|█████▉    | 59/100 [00:47<00:31,  1.29it/s]\r 60%|██████    | 60/100 [00:48<00:31,  1.29it/s]\r 61%|██████    | 61/100 [00:48<00:30,  1.29it/s]\r 62%|██████▏   | 62/100 [00:49<00:29,  1.28it/s]\r 63%|██████▎   | 63/100 [00:50<00:28,  1.28it/s]\r 64%|██████▍   | 64/100 [00:51<00:28,  1.28it/s]\r 65%|██████▌   | 65/100 [00:51<00:27,  1.29it/s]\r 66%|██████▌   | 66/100 [00:52<00:26,  1.28it/s]\r 67%|██████▋   | 67/100 [00:53<00:25,  1.27it/s]\r 68%|██████▊   | 68/100 [00:54<00:25,  1.27it/s]\r 69%|██████▉   | 69/100 [00:55<00:24,  1.27it/s]\r 70%|███████   | 70/100 [00:55<00:23,  1.28it/s]\r 71%|███████   | 71/100 [00:56<00:22,  1.28it/s]\r 72%|███████▏  | 72/100 [00:57<00:21,  1.28it/s]\r 73%|███████▎  | 73/100 [00:58<00:21,  1.28it/s]\r 74%|███████▍  | 74/100 [00:58<00:20,  1.28it/s]\r 75%|███████▌  | 75/100 [00:59<00:19,  1.28it/s]\r 76%|███████▌  | 76/100 [01:00<00:18,  1.28it/s]\r 77%|███████▋  | 77/100 [01:01<00:18,  1.27it/s]\r 78%|███████▊  | 78/100 [01:02<00:17,  1.27it/s]\r 79%|███████▉  | 79/100 [01:02<00:16,  1.26it/s]\r 80%|████████  | 80/100 [01:03<00:15,  1.26it/s]\r 81%|████████  | 81/100 [01:04<00:14,  1.27it/s]\r 82%|████████▏ | 82/100 [01:05<00:14,  1.28it/s]\r 83%|████████▎ | 83/100 [01:06<00:13,  1.28it/s]\r 84%|████████▍ | 84/100 [01:06<00:12,  1.28it/s]\r 85%|████████▌ | 85/100 [01:07<00:11,  1.28it/s]\r 86%|████████▌ | 86/100 [01:08<00:10,  1.29it/s]\r 87%|████████▋ | 87/100 [01:09<00:10,  1.29it/s]\r 88%|████████▊ | 88/100 [01:09<00:09,  1.28it/s]\r 89%|████████▉ | 89/100 [01:10<00:08,  1.28it/s]\r 90%|█████████ | 90/100 [01:11<00:07,  1.28it/s]\r 91%|█████████ | 91/100 [01:12<00:07,  1.28it/s]\r 92%|█████████▏| 92/100 [01:13<00:06,  1.26it/s]\r 93%|█████████▎| 93/100 [01:13<00:05,  1.26it/s]\r 94%|█████████▍| 94/100 [01:14<00:04,  1.25it/s]\r 95%|█████████▌| 95/100 [01:15<00:03,  1.26it/s]\r 96%|█████████▌| 96/100 [01:16<00:03,  1.27it/s]\r 97%|█████████▋| 97/100 [01:17<00:02,  1.27it/s]\r 98%|█████████▊| 98/100 [01:17<00:01,  1.27it/s]\r 99%|█████████▉| 99/100 [01:18<00:00,  1.27it/s]\r100%|██████████| 100/100 [01:19<00:00,  1.28it/s]\r100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n",
            "[INFO|modelcard.py:452] 2024-02-12 03:19:39,766 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'dataset': {'name': 'wmt16 ro-en', 'type': 'wmt16', 'args': 'ro-en'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSpeed ZeRO-3 模式单 GPU 训练翻译模型（T5-11B）\n",
        "%%bash\n",
        "\n",
        "cd transformers; export BS=20; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_flan_t5_z3_offload.json \\\n",
        "--model_name_or_path t5-11b  \\\n",
        "--per_device_train_batch_size $BS \\\n",
        "--per_device_eval_batch_size $BS \\\n",
        "--output_dir /content/drive/MyDrive/models/t5-11b/ --overwrite_output_dir \\\n",
        "--do_train \\\n",
        "--do_eval \\\n",
        "--max_train_samples 500 --num_train_epochs 1 \\\n",
        "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
        "--source_lang en --target_lang ro \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--bf16"
      ],
      "metadata": {
        "id": "k7bkKEx-O0l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cd transformers; export BS=16; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--deepspeed /content/drive/MyDrive/data/deepspeed/config/ds_config_zero3.json \\\n",
        "--model_name_or_path t5-11b  \\\n",
        "--output_dir /content/drive/MyDrive/models/t5-11b/ --adam_eps 1e-06 --evaluation_strategy=steps \\\n",
        "--do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5  \\\n",
        "--max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir  \\\n",
        "--per_device_train_batch_size $BS --per_device_eval_batch_size $BS --predict_with_generate --sortish_sampler \\\n",
        "--val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_eval_samples 500 \\\n",
        "--dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_lang en --target_lang ro \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--fp16\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KWxi27sVJql",
        "outputId": "496432de-3415-4aaf-a17a-43368f70675b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ]
    }
  ]
}